## [밑바닥부터 시작하는 딥러닝]
---
### 목차
- [3.1 퍼셉트론에서 신경망으로](https://github.com/minchan5224/TIL/blob/main/Python/Deep_Learning/01/Deep_01_02.md#21-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0%EC%9D%B4%EB%9E%80)
---
#### 2021_03_06
#### CHAPTER3_신경망
---
> #### 3.1 퍼셉트론에서 신경망으로
> #### 3.1.1 신경망의 예
>> 신경망을 그림을 나타내면 아래 그림과 같다.
>> 
>> ![2층신경망](./image/03/2층신경망.png)
>> 
>> 은닉층의 뉴런은 입력층이나 출력증과 달리 사람의 눈에 보이지 않는다.(책에서는 왼쪽부터 0층 1층 2층)
>> 
>> 지금까지 본 퍼셉트론과 크게 달라 보이지 않는다.(3층으로 구성되지만 가중치를 갖는 층은 2개뿐이라 2층 신경망이라 한다.
>> 
>> 앞에서 살펴본 [퍼셉트론](https://github.com/minchan5224/TIL/blob/main/Python/Deep_Learning/Deep_01_02.md#21-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0%EC%9D%B4%EB%9E%80)엔 편향이 없다.
>> 
>> 해당 네트워크에 편향을 명시하면 아래 그림과 같다.
>> 
>> ![편향을_명시한_퍼셉트론](./image/03/편향을_명시한_퍼셉트론.png)
>> 
>> 가중치가 b이고 입력인 1인 뉴런이 추가 되었다. 
>> 
>> 해당 퍼셉트론의 동작은 x1,x2,1이라는 3개의 신호가 뉴런에 입력되어 각 신호에 가중치를 곱한 후 다음 뉴런에 전달한다.
>> 
>> 다음 뉴런에서는 이 신호들의 값을 더해 합이 0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력한다.
>> 
>> 해당 뉴런을 식으로 표현하면 다음과 같다.
>> 
>> ![식3_2](./image/03/식3_2.png)
>> 
>> 1번 식은 입력 신호의 총 합이 h(x)라는 함수를 거쳐 변환되며, 그 변환된 값이 y의 출력이 됨을 보여준다.
>> 
>> 2번 식 h(x)함수는 입력이 0을 넘으면 1을 돌려주고 아니라면 0을 반환한다. 결과적으로 두 식은 같다.
>
> #### 3.1.3 활성화 함수의 등장
>> 바로전 h(x)라는 함수를 처음 봤다. 이와같이 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 **활성화** **함수** 라고한다.
>> 
>> 활성화 라는 이름에서 알 수 있듯 활성화 함수는 입력 신호의 총합이 활성화를 일이키는지 정하는 역할을 한다. 위에서 사용했던 식을 아래와 같이 새로 작성했다.
>> 
>> ![식3_4](./image/03/식3_4.png)
>> 
>> 1번 식은 가중치가 달린 입력 신호와 편향의 총합을 계산하고 a라고 한다.
>> 
>> 2번 식은 a를 함수 h()에 넣어 출력하는 흐름이다.
>> 
>> 지금까지와 같이 뉴런을 원을 이용해 그리면 위 식은 아래 그림과 같이 나타낼 수 있다.
>> 
>> ![활성화_함수의_처리_과정](./image/03/활성화_함수의_처리_과정.png)
>> 
>> 그림과 같이 기존 뉴런의 원을 키우고 그 안에 활성화 함수의 처리과정을 명시적으로 그려 넣었다.
>> 
>> 즉 가중치 신호를 조합한 결과가 a라는 노드가 되고, 활성화 함수h()를 통과하여 y라는 노드로 변환되는 과정을 분명히 나타낸 것이다.
>> - 이 책에서는 노드 == 뉴런
> 
> #### 3.1 활성화 함수
> 활성화 함수는 임계값을 경계로 출력이 바뀐다. 이러한 함수를 **계단** **함수**라 한다.
> 그래서 퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다 라고 할 수 있다.
> - 활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아가는 열쇠라고 한다.
> 
> #### 3.2.1 시그모이드 함수
>> 시그모이드 함수는 신경망에서 자주 이용하는 활성화 함수이다.
>> 
>> ![시그모이드_식](./image/03/시그모이드_식.png)
>> 
>> 위 식은 시그모이드 함수의 식이며 exp(-x)는 e^(-x)를 뜻한다.
>> - 식만 본다면 어려워 보이지만 단순한 '함수'다. 입력을 주면 출력을 반환한다.
>> 
> #### 3.2.2 계단 함수 구현하기
> 파이썬을 이용해 계단 함tn를 그려본다.
> ```Python
> def step_function(x):
>     if x > 0:
>         return 1
>     else :
>         return 0
> ```
>  - 계단 함수는 입력이 0을 넘으면 1을 출력하고 그 외에는 전부 0을 출력하는 함수다.
> 위 코드는 구현하기 간단하지만 인수 x는 실수(부동소수점)만 받아들인다.
> 
> (3.0)은 가능하지만 넘파이 배열(np.array([1.0, 2.0]))을 인수로 넘길 수 없다. 
> 
> 넘파이 배열도 사용하기 위해선 아래와 같이 수정 한다.
> ```Python
> def step_function(x):
>     y = x > 0
>     return y.astype(np.int)
> ```
> 두 줄인 코드로 충분하다. 넘파이 배열에 부등호 연산을 수행하면 배열의 원소 각각에 부등호 연산을 수행한 bool배열이 생성된다.
> 
> 이 코드에선 0보다 클때 True를 반환한다. 따라서 True, False로 변환한 배열이 생성된다. [False, True, True]
> 
> 그 후 bool형 데이터를 int형으로 변환하면 False는 0, True는 1로 변환됨을 알 수 있다.  [0, 1, 1]
> 
> #### 3.2.3 계단 함수의 그래프
> 파이썬의 matplotlib라이브러리를 이용해서 그래프를 그린다.
> ```Python
> import numpy as np
> import matplotlib.pylab as plt
> 
> def step_function(x):
>     return np.array(x > 0, dtype=np.int)
> x = np.arange(-5.0, 5.0, 0.1) # -5.0~5.0 전까지 0.1 간격의 넘파이 배열 생성 [-5.0, -4.0,~,4.9]
> y = step_function(x)
> plt.plot(x, y)
> plt.plot(-0.1, 1.1) # y축 범위 지정
> plt.show()
> ```
> 해당 코드의 실행 결과는 아래 그림과 같다.
>> ![계단_함수_그래프](./image/03/계단_함수_그래프.png)
> 
> 그림과 같이 계단 함수는 0을 경계로 출력이 1과 0으로 나뉜다.
> 
> #### 3.2.4 시그모이드 함수 구현하기
> 이번엔 시그모이드 함수를 파이썬을 이용해 구현한다.
> ```Python
> def sigmoid(x):
>     return 1 / (1 + np.exp(-x)) # np.exp(-x)는 exp(-x)수식에 해당한다.
> ```
> x = np.array([-1.0, 1.0, 2.0]) 일때 sigmoid(x)를 한다면 넘파이의 브로드캐스트로 인해 정확하게 동작한다.
> - 브로드캐스트 : 넘파이 배열과 스칼라값의 연산을 넘파이 배열의 원소 각각과 스칼라 값의 연산으로 바꿔 수행하는 것
> 
> 아래 코드를 이용해 시그모이드 함수의 그래프를 그린다.
> ```Python
> def sigmoid(x):
>     return 1 / (1 + np.exp(-x)) 
>
> x = np.arange(-5.0, 5.0, 0.1)
> y = sigmoid(x)
> plt.plot(x, y)
> plt.plot(-0.1, 1.1) # y축 범위 지정
> plt.show()
> ```
> 해당 코드의 실행 결과는 아래 그림과 같다.
>
> ![시그모이드_그래프](./image/03/시그모이드_그래프.png)
> 
> #### 3.2.4 시그모이드 함수와 계단 함수 비교
> 
> ![시그모이드_계단](./image/03/시그모이드_계단.png)
> 
> 같이보니 확실한 차이가 보인다. 곡선과 직선의 차이라고 할 수 있는것 같다.
> 
> 시그모이드의 매끈함이 신경망 학습에서 아주 중요한 역할을 하게 된다고 한다.
> 
> 계단함수는 1과 0만 돌려주는 것에 비해 시그모이드 함수는 실수를 포함해 돌려주는 차이가 있다.
> 
> 또한 입력이 커지면 1에 가까운 혹은 1을 출력하는 점( 입력이 중요하면 큰 값을 출력하고 중요하지 않다면 작은 값을 출력한다)과 출력은 0과 1 사이라는 점이 둘의 공통점 이다.
> 
> #### 3.2.6 비선형 함수
> 계단 함수와 시그모이드 함수 모두 **비선형** **함수**이다.
> 
> 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다.
> - 선형 함수를 사용하면 신경망의 층을 깊게하는 이유가 없어지기 때문이다.
>> 선형 함수로 층을 아무리 깊게 해도 **은닉층이** **없는** **네트워크**로 똑같은 기능을 할 수 있기 때문이다.
>> 
>> h(x) = cx를 활성화 함수로 사용한 3층 네트워크를 예로 들었을 때 
>> 
>> 식으로 나타낸다면 y(x) = h(h(h(x)))이다. 이 계산은 y(x) = c * c * c * x 와 같이 곱셈을 세번 수행하지만 
>> 
>> y(x) = ax와 같은 식이다, a=c^3이라고 하면 되기 때문. 즉 은닉층이 없는 네트워크로 표현이 가능하다.
>> 
>> 따라서 층을 쌓는 혜택을 얻기 위해선 활성화 함수로 비선형 함수를 사용해야한다.
> 
> #### 3.2.7 ReLU 함수
> 최근에는 활성화 함수로 ReLU함수를 주로 이용한다.
> 
> ![ReLU_함수](./image/03/ReLU_함수.png)
> 
> ReLU 함수는 그림과 같이 입력이 0을 넘으면 그대로 출력하고 0 이하면 0을 출력한다. 식은 아래와 같다.
> 
> ![ReLU_식](./image/03/ReLU_식.png)
> 
> 그림과 식을 보듯이 간단한 함수이며 파이썬으로 구현한다면 코드는 아래와 같다.
> ```Python
> def relu(x):
>     return np.maximum(0, x)
> ```
> 해당 코드에서는 넘파이의 maximum()을 사용하며 두 입력중 큰 값을 선택해 반환하는 함수다.
> 
> 따라서 0보다 작을 땐 정상적으로 0, 0일때도 0을 출력한다 0보다 클때만 x를 출력한다.
> 
---
#### 2021_03_06
> #### 3.3 다차원 배열의 계산 
> 3.3.1 다차원 배열
> ```Python
> B = np.array([[1,2], [3,4], [5,6]]) # 행렬 생성
> print(B)
> [[1 2]
>  [3 4]
>  [5 6]]
>  
> np.ndim(B) # 배열의 차원수 확인
> 2
>  
> B.shape # 배열의 형상 확인(3X2배열)
> (3, 2)
> ```
> 기본적으로 행렬의 가로는 행 세로를 열이라고 한다 (행거는 가로 열) 
> 
> 행렬의 곱 계산 방법은 아래 그림과 같다.
> 
> ![행렬_곱_계산](./image/03/행렬_곱_계산.png)
> 
> ```Python
> A = np.array([[1,2], [3,4]])
> B = np.array([[5,6], [7,8]])
> np.dot(A, B) # 행렬의 곱 계산
> 
> array([[19, 22], # 결과
>        [43, 50]])
> ```
> 위와같이 np.dot()함수를 이용해 행렬의 곱을 계산 가능하다.
> - np.dot()는 입력이 1차원 배열이면 벡터를 2차원 배열이면 행렬의 곱을 계산한다.
> 
> - 또한 행렬의 곱 연산 특성상 A와 B의 순서가 바뀌면 결과 또한 바뀐다.
> 
> ![대응하는_행렬_차원의_원소_수_일치](./image/03/대응하는_행렬_차원의_원소_수_일치.png)
> 
> ![차원_수가_달라도_대응하는_차원의_원소_수_일치](./image/03/차원_수가_달라도_대응하는_차원의_원소_수_일치.png)
> 
> 위의 그림과 같이 행렬의 곱을 계산할 때는 대응하는 차원의 원소 수를 일치시켜야한다.
> 
> 행렬의 차원이 다르더라도 대응하는 차원의 원소수를 칠치시켜 곱을 한다.
---

