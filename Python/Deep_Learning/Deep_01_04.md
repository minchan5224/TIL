## [밑바닥부터 시작하는 딥러닝]
---
### 목차
- [4.1 제목](https://github.com/minchan5224/TIL/blob/main/Python/Deep_Learning/01/Deep_01_02.md#21-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0%EC%9D%B4%EB%9E%80)
---
#### 2021_3_18~
#### CHAPTER4_신경망 학습
---
> #### 4.1 데이터에서 학습한다.
> 신경망의 특징은 데이터를 보고 학습할 수 있다는 점이다.
> 
> 데이터에서 학습한다는 것은 **가중치** **매개변수의** **값을** **데이터를** **보고** **자동으로** **결정한다**는 뜻이다.
> - 실제 신경망에서 매개변수는 수천에서 수만개다. 층을 깊게한 딥러닝의 경우는 수억개다. 자동으로 결정한다니 희소식이다.
> 
> [퍼셉트론 수렴 정리](https://nbviewer.jupyter.org/github/metamath1/ml-simple-works/blob/master/perceptron/perceptron.ipynb)
> 
> #### 4.1.1 데이터 주도 학습
> 기계학습은 데이터가 생명이다
> - 데이터가 없으면 아무것도 시작되지 않기때문이다.
> 
> - 데이터가 이끄는 접근 방식 덕에 사람 중심 접근에서 벗어날 수 있다.
> 
> 보통은 문제를 해결할 때, 특히 어떤 패턴을 찾아야 할 때는 사람이 주도하는것이 대부분이지만 기계학습에선 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도한다.
> 
> 또한 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지녔다.
> - 이미지에서 **특징**을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다.
> 
> - 여기서 말하는 **특징**은 입력 데이터(이미지)에서 본질적인 데이터를 정확하게 추출할 수 있도록 설계된 변환기를 가리킨다.
>> 이미지의 특징은 보톡 벡터로 기술하고, 컴퓨터 비전 분야에서는 [SIFT](https://ballentain.tistory.com/47), [SURF](https://hello-stella.tistory.com/23), [HOG](http://blog.naver.com/PostView.nhn?blogId=tommybee&logNo=221173056260&parentCategoryNo=&categoryNo=57&viewDate=&isShowPopularPosts=true&from=search) 등의 특징을 많이 사룔한다.
>> 
>> 이런 특징을 사용해 이미지 데이터를 벡터로 변환하고 변환된 벡터를 가지고 지도 학습의 대표 분류 기법인 [SVM](https://ko.wikipedia.org/wiki/%EC%84%9C%ED%8F%AC%ED%8A%B8_%EB%B2%A1%ED%84%B0_%EB%A8%B8%EC%8B%A0), [KNN](https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98) 등으로 학습할 수 있다.
>
> 이와 같이 기계학습에선 모아진 데이터로부터 규칙을 찾아내는 역할을 기계가 담당한다.
> 
> 다만 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 만든다. 
> 
> 좋은 결과를 얻기 위해선 문제에 적합한 특징을 잘 정해서 사용해야 한다는 것이다. 
> 
> ![그림_4_2](./image/04/그림_4_2.png)
> - 회색 블록은 사람이 개입하지 않는 것을 뜻함
> 
> 위 그림과 같이 신경망은 이미지를 있는 그래로 학습한다.
> 
> 두 번째 접근방식(특징, 기계학습 방식) 에서는 특징을 사람이 설계했지만 신경망은 이미지에 포함된 중요한 특징까지도 '기계'가 스스로 학습할 것이다.
> - 딥러닝을 **종단간** **기계학습**이라고도 한다, 종단간은 처음부터 끝까지를 의미하며 데이터 입력에서 부터 목표로한 결과 출력 까지를 사람의 개입 없이 진행한다는 뜻을 가진다.
> 
> 신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점에 있다.
> - 세부사항과 관계없이 신경망은 주어진 데이터를 온전히 학습하고, 주어진 문제의 패턴을 발견하려 시도한다. 즉, 신경망은 모든 문제를 주어진 데이터 그대로 입력 데이터로 활용해 'end-to-end'로 학습할 수 있다
> 
> #### 4.1.2 훈련 데이터와 시험 데이터
> 기계학습 문제는 데이터를 **훈련 데이터**와 **시험 데이터**로 나누어 학습과 실험을 수행하는 것이 일반적이다.
> 
> 우선 훈련 데이터만 이용해 학습하며 최적의 매개변수를 찾는다. 그 다음 시험 데이터를 이용해 앞서 훈련한 모델의 실력을 평가한다.
> 
> 범용적으로 사용할 수 있는 모델을 원하기 때문에 데이터를 훈련과 시험으로 나누어 사용한다.
>> 범용 능력은 아직 보지 못한 데이터(훈련 데이터에 포함되지 않는 데이터)로도 문제를 올바르게 풀어내는 능력이다.
>> 
>> 또한 범용 능력을 획득하는 것이 기계학습의 최종 목표다.
>> 
>> 데이터셋 하나로만 매개변수의 학습과 평가를 수행하면 올바른 평가가 될 수 없다
>> - 훈련 데이터와 시험 데이터가 같다면 해당 데이터에 포함된 자료들만 학습한 것이다. 제대로된 평가를 할 수 없다.(다른 데이터셋에서 엉망일 가능성이 생긴다)
>> 
>> 또한 하나의 데이터셋에만 지나치게 최적화된 상태를 **오버피팅**이라 하며 오버피팅을 피하는 것은 기계학습의 중요한 과제이기도 하다.
>
> #### 4.2 손실 함수
> 신경망에서는 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색한다.
> 
> 신경망 학습에서 사용하는 지표는 **손실 함수** 라고 하며, 손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 오차제곱합과 교차 엔트로피 오차를 사용한다.
> 
> #### 4.2.1 오차제곱합
> ![식4_1](./image/04/식4_1.png) 
> 
> 위 식은 오차제곱합의 수식이다.
> 
> 여기서 Yk는 신경망의 출력(신경망이 추정한 값), Tk는 정답 레이블, k는 데이터의 차원 수를 나타낸다.
> 
> ```Python
> y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
> t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
> ```
> 위 배열들의 원소는 첫 번째 인덱스부터 순서대로 0~9일 때의 값이다.
> 
> 여기서 신경망의 출력 y는 소프트맥스 함수의 출력이다. 소프트맥스 함수의 출력은 확률로 해석이 가능하다(전체 합이 1이다.)
> 
> 위의 예에선 2번 인덱스이 값이 0.6 즉 2일 확률이 60%라고 해석 할 수 있다
> 
> 또한 정답 레이블인 t는 정답을 기리키는 원소는 1, 나머지는 0으로 표시한다(one_hot_label), 2에 해당하는 인덱스에 있는 원소의 값이 1이므로 정답이 2인 것을 알 수 있다.
> 
> 위 과정은 오차제곱합을 구하기 위해 복습한 것이다
> 
> 오차제곱합은 바로 위의 그림의 식과 같이 각 원소의 출력(추정 값)과 정답 에이블(참 값)의 차(Yk-Tk)를 제곱한 후 그 총합을 구한다.
> ```Python
> def sum_squares_error(y, t):
>     return 0.5 * np.sum((y-t)**2)
> ```
> 위 코드는 파이썬을 이용해 오차제곱합을 구현한 것이다.
> 
> 여기서 인수 y와 t는 넘파이 배열바로 사용해 본다.
> ```Python
> t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답 2
> y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 2의 확률이 가장 높게 추정하였다.
> 
> sum_squares_error(np.array(y), np.array(t)) # 0.097500000000000031
> 
> y = [ 0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]# 7의 확률이 가장 높게 추정하였다.
> sum_squares_error(np.array(y), np.array(t)) # 0.59750000000000003
> ```
> 두가지 예시를 봤다 첫 번째 예시는 정답과 신경망 출력이 모두 2에서 가장 높은 경우다.
> 
> 두 번째 예시는 정답은 마찬가지로 2이지만 신경망 출력이 7에서 가장 높을 때다.
> 
> 첫 번째 예의 손실 함수 쪽 출력이 작그며 정답 레이블과의 오차 또한 작은 것을 알 수 있다. 즉 오차제곱합 기준으로는 첫 번째 추정 결과가(오차가 더 작으므로) 정답에 가까울 것으로 판단할 수 있다.
> 
> #### 4.2.2 교차 엔트로피 오차
