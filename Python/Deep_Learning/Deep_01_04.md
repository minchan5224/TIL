## [ë°‘ë°”ë‹¥ë¶€í„° ì‹œìž‘í•˜ëŠ” ë”¥ëŸ¬ë‹]
---
### ëª©ì°¨
- [4.1 ì œëª©](https://github.com/minchan5224/TIL/blob/main/Python/Deep_Learning/01/Deep_01_02.md#21-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0%EC%9D%B4%EB%9E%80)
- [4.2 ì†ì‹¤ í•¨ìˆ˜](https://github.com/minchan5224/TIL/blob/main/Python/Deep_Learning/Deep_01_04.md#42-%EC%86%90%EC%8B%A4-%ED%95%A8%EC%88%98)
- [4.3 ìˆ˜ì¹˜ ë¯¸ë¶„](https://github.com/minchan5224/TIL/blob/main/Python/Deep_Learning/Deep_01_04.md#43-%EC%88%98%EC%B9%98-%EB%AF%B8%EB%B6%84)
- [4.4 ê¸°ìš¸ê¸°](https://github.com/minchan5224/TIL/blob/main/Python/Deep_Learning/Deep_01_04.md#44-%EA%B8%B0%EC%9A%B8%EA%B8%B0)
---
#### CHAPTER4_ì‹ ê²½ë§ í•™ìŠµ
##### 2021_3_18
---
> #### 4.1 ë°ì´í„°ì—ì„œ í•™ìŠµí•œë‹¤.
> ì‹ ê²½ë§ì˜ íŠ¹ì§•ì€ ë°ì´í„°ë¥¼ ë³´ê³  í•™ìŠµí•  ìˆ˜ ìžˆë‹¤ëŠ” ì ì´ë‹¤.
> 
> ë°ì´í„°ì—ì„œ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì€ **ê°€ì¤‘ì¹˜** **ë§¤ê°œë³€ìˆ˜ì˜** **ê°’ì„** **ë°ì´í„°ë¥¼** **ë³´ê³ ** **ìžë™ìœ¼ë¡œ** **ê²°ì •í•œë‹¤**ëŠ” ëœ»ì´ë‹¤.
> - ì‹¤ì œ ì‹ ê²½ë§ì—ì„œ ë§¤ê°œë³€ìˆ˜ëŠ” ìˆ˜ì²œì—ì„œ ìˆ˜ë§Œê°œë‹¤. ì¸µì„ ê¹Šê²Œí•œ ë”¥ëŸ¬ë‹ì˜ ê²½ìš°ëŠ” ìˆ˜ì–µê°œë‹¤. ìžë™ìœ¼ë¡œ ê²°ì •í•œë‹¤ë‹ˆ í¬ì†Œì‹ì´ë‹¤.
> 
> [í¼ì…‰íŠ¸ë¡  ìˆ˜ë ´ ì •ë¦¬](https://nbviewer.jupyter.org/github/metamath1/ml-simple-works/blob/master/perceptron/perceptron.ipynb)
> 
> #### 4.1.1 ë°ì´í„° ì£¼ë„ í•™ìŠµ
> ê¸°ê³„í•™ìŠµì€ ë°ì´í„°ê°€ ìƒëª…ì´ë‹¤
> - ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ ì‹œìž‘ë˜ì§€ ì•Šê¸°ë•Œë¬¸ì´ë‹¤.
> 
> - ë°ì´í„°ê°€ ì´ë„ëŠ” ì ‘ê·¼ ë°©ì‹ ë•ì— ì‚¬ëžŒ ì¤‘ì‹¬ ì ‘ê·¼ì—ì„œ ë²—ì–´ë‚  ìˆ˜ ìžˆë‹¤.
> 
> ë³´í†µì€ ë¬¸ì œë¥¼ í•´ê²°í•  ë•Œ, íŠ¹ížˆ ì–´ë–¤ íŒ¨í„´ì„ ì°¾ì•„ì•¼ í•  ë•ŒëŠ” ì‚¬ëžŒì´ ì£¼ë„í•˜ëŠ”ê²ƒì´ ëŒ€ë¶€ë¶„ì´ì§€ë§Œ ê¸°ê³„í•™ìŠµì—ì„  ì‚¬ëžŒì˜ ê°œìž…ì„ ìµœì†Œí™”í•˜ê³  ìˆ˜ì§‘í•œ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ ì°¾ìœ¼ë ¤ ì‹œë„í•œë‹¤.
> 
> ë˜í•œ ì‹ ê²½ë§ê³¼ ë”¥ëŸ¬ë‹ì€ ê¸°ì¡´ ê¸°ê³„í•™ìŠµì—ì„œ ì‚¬ìš©í•˜ë˜ ë°©ë²•ë³´ë‹¤ ì‚¬ëžŒì˜ ê°œìž…ì„ ë”ìš± ë°°ì œí•  ìˆ˜ ìžˆê²Œ í•´ì£¼ëŠ” ì¤‘ìš”í•œ íŠ¹ì„±ì„ ì§€ë…”ë‹¤.
> - ì´ë¯¸ì§€ì—ì„œ **íŠ¹ì§•**ì„ ì¶”ì¶œí•˜ê³  ê·¸ íŠ¹ì§•ì˜ íŒ¨í„´ì„ ê¸°ê³„í•™ìŠµ ê¸°ìˆ ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ ìžˆë‹¤.
> 
> - ì—¬ê¸°ì„œ ë§í•˜ëŠ” **íŠ¹ì§•**ì€ ìž…ë ¥ ë°ì´í„°(ì´ë¯¸ì§€)ì—ì„œ ë³¸ì§ˆì ì¸ ë°ì´í„°ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•  ìˆ˜ ìžˆë„ë¡ ì„¤ê³„ëœ ë³€í™˜ê¸°ë¥¼ ê°€ë¦¬í‚¨ë‹¤.
>> ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì€ ë³´í†¡ ë²¡í„°ë¡œ ê¸°ìˆ í•˜ê³ , ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œëŠ” [SIFT](https://ballentain.tistory.com/47), [SURF](https://hello-stella.tistory.com/23), [HOG](http://blog.naver.com/PostView.nhn?blogId=tommybee&logNo=221173056260&parentCategoryNo=&categoryNo=57&viewDate=&isShowPopularPosts=true&from=search) ë“±ì˜ íŠ¹ì§•ì„ ë§Žì´ ì‚¬ë£”í•œë‹¤.
>> 
>> ì´ëŸ° íŠ¹ì§•ì„ ì‚¬ìš©í•´ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  ë³€í™˜ëœ ë²¡í„°ë¥¼ ê°€ì§€ê³  ì§€ë„ í•™ìŠµì˜ ëŒ€í‘œ ë¶„ë¥˜ ê¸°ë²•ì¸ [SVM](https://ko.wikipedia.org/wiki/%EC%84%9C%ED%8F%AC%ED%8A%B8_%EB%B2%A1%ED%84%B0_%EB%A8%B8%EC%8B%A0), [KNN](https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98) ë“±ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìžˆë‹¤.
>
> ì´ì™€ ê°™ì´ ê¸°ê³„í•™ìŠµì—ì„  ëª¨ì•„ì§„ ë°ì´í„°ë¡œë¶€í„° ê·œì¹™ì„ ì°¾ì•„ë‚´ëŠ” ì—­í• ì„ ê¸°ê³„ê°€ ë‹´ë‹¹í•œë‹¤.
> 
> ë‹¤ë§Œ ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•  ë•Œ ì‚¬ìš©í•˜ëŠ” íŠ¹ì§•ì€ ì—¬ì „ížˆ ì‚¬ëžŒì´ ë§Œë“ ë‹¤. 
> 
> ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ì„  ë¬¸ì œì— ì í•©í•œ íŠ¹ì§•ì„ ìž˜ ì •í•´ì„œ ì‚¬ìš©í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. 
> 
> ![ê·¸ë¦¼_4_2](./image/04/ê·¸ë¦¼_4_2.png)
> - íšŒìƒ‰ ë¸”ë¡ì€ ì‚¬ëžŒì´ ê°œìž…í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ëœ»í•¨
> 
> ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ì‹ ê²½ë§ì€ ì´ë¯¸ì§€ë¥¼ ìžˆëŠ” ê·¸ëž˜ë¡œ í•™ìŠµí•œë‹¤.
> 
> ë‘ ë²ˆì§¸ ì ‘ê·¼ë°©ì‹(íŠ¹ì§•, ê¸°ê³„í•™ìŠµ ë°©ì‹) ì—ì„œëŠ” íŠ¹ì§•ì„ ì‚¬ëžŒì´ ì„¤ê³„í–ˆì§€ë§Œ ì‹ ê²½ë§ì€ ì´ë¯¸ì§€ì— í¬í•¨ëœ ì¤‘ìš”í•œ íŠ¹ì§•ê¹Œì§€ë„ 'ê¸°ê³„'ê°€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•  ê²ƒì´ë‹¤.
> - ë”¥ëŸ¬ë‹ì„ **ì¢…ë‹¨ê°„** **ê¸°ê³„í•™ìŠµ**ì´ë¼ê³ ë„ í•œë‹¤, ì¢…ë‹¨ê°„ì€ ì²˜ìŒë¶€í„° ëê¹Œì§€ë¥¼ ì˜ë¯¸í•˜ë©° ë°ì´í„° ìž…ë ¥ì—ì„œ ë¶€í„° ëª©í‘œë¡œí•œ ê²°ê³¼ ì¶œë ¥ ê¹Œì§€ë¥¼ ì‚¬ëžŒì˜ ê°œìž… ì—†ì´ ì§„í–‰í•œë‹¤ëŠ” ëœ»ì„ ê°€ì§„ë‹¤.
> 
> ì‹ ê²½ë§ì˜ ì´ì ì€ ëª¨ë“  ë¬¸ì œë¥¼ ê°™ì€ ë§¥ë½ì—ì„œ í’€ ìˆ˜ ìžˆë‹¤ëŠ” ì ì— ìžˆë‹¤.
> - ì„¸ë¶€ì‚¬í•­ê³¼ ê´€ê³„ì—†ì´ ì‹ ê²½ë§ì€ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì˜¨ì „ížˆ í•™ìŠµí•˜ê³ , ì£¼ì–´ì§„ ë¬¸ì œì˜ íŒ¨í„´ì„ ë°œê²¬í•˜ë ¤ ì‹œë„í•œë‹¤. ì¦‰, ì‹ ê²½ë§ì€ ëª¨ë“  ë¬¸ì œë¥¼ ì£¼ì–´ì§„ ë°ì´í„° ê·¸ëŒ€ë¡œ ìž…ë ¥ ë°ì´í„°ë¡œ í™œìš©í•´ 'end-to-end'ë¡œ í•™ìŠµí•  ìˆ˜ ìžˆë‹¤
> 
> #### 4.1.2 í›ˆë ¨ ë°ì´í„°ì™€ ì‹œí—˜ ë°ì´í„°
> ê¸°ê³„í•™ìŠµ ë¬¸ì œëŠ” ë°ì´í„°ë¥¼ **í›ˆë ¨ ë°ì´í„°**ì™€ **ì‹œí—˜ ë°ì´í„°**ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµê³¼ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.
> 
> ìš°ì„  í›ˆë ¨ ë°ì´í„°ë§Œ ì´ìš©í•´ í•™ìŠµí•˜ë©° ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì°¾ëŠ”ë‹¤. ê·¸ ë‹¤ìŒ ì‹œí—˜ ë°ì´í„°ë¥¼ ì´ìš©í•´ ì•žì„œ í›ˆë ¨í•œ ëª¨ë¸ì˜ ì‹¤ë ¥ì„ í‰ê°€í•œë‹¤.
> 
> ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ëª¨ë¸ì„ ì›í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„°ë¥¼ í›ˆë ¨ê³¼ ì‹œí—˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‚¬ìš©í•œë‹¤.
>> ë²”ìš© ëŠ¥ë ¥ì€ ì•„ì§ ë³´ì§€ ëª»í•œ ë°ì´í„°(í›ˆë ¨ ë°ì´í„°ì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ë°ì´í„°)ë¡œë„ ë¬¸ì œë¥¼ ì˜¬ë°”ë¥´ê²Œ í’€ì–´ë‚´ëŠ” ëŠ¥ë ¥ì´ë‹¤.
>> 
>> ë˜í•œ ë²”ìš© ëŠ¥ë ¥ì„ íšë“í•˜ëŠ” ê²ƒì´ ê¸°ê³„í•™ìŠµì˜ ìµœì¢… ëª©í‘œë‹¤.
>> 
>> ë°ì´í„°ì…‹ í•˜ë‚˜ë¡œë§Œ ë§¤ê°œë³€ìˆ˜ì˜ í•™ìŠµê³¼ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ë©´ ì˜¬ë°”ë¥¸ í‰ê°€ê°€ ë  ìˆ˜ ì—†ë‹¤
>> - í›ˆë ¨ ë°ì´í„°ì™€ ì‹œí—˜ ë°ì´í„°ê°€ ê°™ë‹¤ë©´ í•´ë‹¹ ë°ì´í„°ì— í¬í•¨ëœ ìžë£Œë“¤ë§Œ í•™ìŠµí•œ ê²ƒì´ë‹¤. ì œëŒ€ë¡œëœ í‰ê°€ë¥¼ í•  ìˆ˜ ì—†ë‹¤.(ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œ ì—‰ë§ì¼ ê°€ëŠ¥ì„±ì´ ìƒê¸´ë‹¤)
>> 
>> ë˜í•œ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ì—ë§Œ ì§€ë‚˜ì¹˜ê²Œ ìµœì í™”ëœ ìƒíƒœë¥¼ **ì˜¤ë²„í”¼íŒ…**ì´ë¼ í•˜ë©° ì˜¤ë²„í”¼íŒ…ì„ í”¼í•˜ëŠ” ê²ƒì€ ê¸°ê³„í•™ìŠµì˜ ì¤‘ìš”í•œ ê³¼ì œì´ê¸°ë„ í•˜ë‹¤.
>
> #### 4.2 ì†ì‹¤ í•¨ìˆ˜
> ì‹ ê²½ë§ì—ì„œëŠ” 'í•˜ë‚˜ì˜ ì§€í‘œ'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ ê°’ì„ íƒìƒ‰í•œë‹¤.
> 
> ì‹ ê²½ë§ í•™ìŠµì—ì„œ ì‚¬ìš©í•˜ëŠ” ì§€í‘œëŠ” **ì†ì‹¤ í•¨ìˆ˜** ë¼ê³  í•˜ë©°, ì†ì‹¤ í•¨ìˆ˜ëŠ” ìž„ì˜ì˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìžˆì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œëŠ” ì˜¤ì°¨ì œê³±í•©ê³¼ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ ì‚¬ìš©í•œë‹¤.
> 
> #### 4.2.1 ì˜¤ì°¨ì œê³±í•©
> ![ì‹4_1](./image/04/ì‹4_1.png) 
> 
> ìœ„ ì‹ì€ ì˜¤ì°¨ì œê³±í•©ì˜ ìˆ˜ì‹ì´ë‹¤.
> 
> ì—¬ê¸°ì„œ YkëŠ” ì‹ ê²½ë§ì˜ ì¶œë ¥(ì‹ ê²½ë§ì´ ì¶”ì •í•œ ê°’), TkëŠ” ì •ë‹µ ë ˆì´ë¸”, këŠ” ë°ì´í„°ì˜ ì°¨ì› ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
> 
> ```Python
> y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
> t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
> ```
> ìœ„ ë°°ì—´ë“¤ì˜ ì›ì†ŒëŠ” ì²« ë²ˆì§¸ ì¸ë±ìŠ¤ë¶€í„° ìˆœì„œëŒ€ë¡œ 0~9ì¼ ë•Œì˜ ê°’ì´ë‹¤.
> 
> ì—¬ê¸°ì„œ ì‹ ê²½ë§ì˜ ì¶œë ¥ yëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì˜ ì¶œë ¥ì´ë‹¤. ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì˜ ì¶œë ¥ì€ í™•ë¥ ë¡œ í•´ì„ì´ ê°€ëŠ¥í•˜ë‹¤(ì „ì²´ í•©ì´ 1ì´ë‹¤.)
> 
> ìœ„ì˜ ì˜ˆì—ì„  2ë²ˆ ì¸ë±ìŠ¤ì´ ê°’ì´ 0.6 ì¦‰ 2ì¼ í™•ë¥ ì´ 60%ë¼ê³  í•´ì„ í•  ìˆ˜ ìžˆë‹¤
> 
> ë˜í•œ ì •ë‹µ ë ˆì´ë¸”ì¸ tëŠ” ì •ë‹µì„ ê¸°ë¦¬í‚¤ëŠ” ì›ì†ŒëŠ” 1, ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ í‘œì‹œí•œë‹¤(one_hot_label), 2ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ì— ìžˆëŠ” ì›ì†Œì˜ ê°’ì´ 1ì´ë¯€ë¡œ ì •ë‹µì´ 2ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìžˆë‹¤.
> 
> ìœ„ ê³¼ì •ì€ ì˜¤ì°¨ì œê³±í•©ì„ êµ¬í•˜ê¸° ìœ„í•´ ë³µìŠµí•œ ê²ƒì´ë‹¤
> 
> ì˜¤ì°¨ì œê³±í•©ì€ ë°”ë¡œ ìœ„ì˜ ê·¸ë¦¼ì˜ ì‹ê³¼ ê°™ì´ ê° ì›ì†Œì˜ ì¶œë ¥(ì¶”ì • ê°’)ê³¼ ì •ë‹µ ì—ì´ë¸”(ì°¸ ê°’)ì˜ ì°¨(Yk-Tk)ë¥¼ ì œê³±í•œ í›„ ê·¸ ì´í•©ì„ êµ¬í•œë‹¤.
> ```Python
> def sum_squares_error(y, t):
>     return 0.5 * np.sum((y-t)**2)
> ```
> ìœ„ ì½”ë“œëŠ” íŒŒì´ì¬ì„ ì´ìš©í•´ ì˜¤ì°¨ì œê³±í•©ì„ êµ¬í˜„í•œ ê²ƒì´ë‹¤.
> 
> ì—¬ê¸°ì„œ ì¸ìˆ˜ yì™€ tëŠ” ë„˜íŒŒì´ ë°°ì—´ë°”ë¡œ ì‚¬ìš©í•´ ë³¸ë‹¤.
> ```Python
> t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # ì •ë‹µ 2
> y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 2ì˜ í™•ë¥ ì´ ê°€ìž¥ ë†’ê²Œ ì¶”ì •í•˜ì˜€ë‹¤.
> 
> sum_squares_error(np.array(y), np.array(t)) # 0.097500000000000031
> 
> y = [ 0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]# 7ì˜ í™•ë¥ ì´ ê°€ìž¥ ë†’ê²Œ ì¶”ì •í•˜ì˜€ë‹¤.
> sum_squares_error(np.array(y), np.array(t)) # 0.59750000000000003
> ```
> ë‘ê°€ì§€ ì˜ˆì‹œë¥¼ ë´¤ë‹¤ ì²« ë²ˆì§¸ ì˜ˆì‹œëŠ” ì •ë‹µê³¼ ì‹ ê²½ë§ ì¶œë ¥ì´ ëª¨ë‘ 2ì—ì„œ ê°€ìž¥ ë†’ì€ ê²½ìš°ë‹¤.
> 
> ë‘ ë²ˆì§¸ ì˜ˆì‹œëŠ” ì •ë‹µì€ ë§ˆì°¬ê°€ì§€ë¡œ 2ì´ì§€ë§Œ ì‹ ê²½ë§ ì¶œë ¥ì´ 7ì—ì„œ ê°€ìž¥ ë†’ì„ ë•Œë‹¤.
> 
> ì²« ë²ˆì§¸ ì˜ˆì˜ ì†ì‹¤ í•¨ìˆ˜ ìª½ ì¶œë ¥ì´ ìž‘ê·¸ë©° ì •ë‹µ ë ˆì´ë¸”ê³¼ì˜ ì˜¤ì°¨ ë˜í•œ ìž‘ì€ ê²ƒì„ ì•Œ ìˆ˜ ìžˆë‹¤. ì¦‰ ì˜¤ì°¨ì œê³±í•© ê¸°ì¤€ìœ¼ë¡œëŠ” ì²« ë²ˆì§¸ ì¶”ì • ê²°ê³¼ê°€(ì˜¤ì°¨ê°€ ë” ìž‘ìœ¼ë¯€ë¡œ) ì •ë‹µì— ê°€ê¹Œìš¸ ê²ƒìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìžˆë‹¤.
> 
---
##### 2021_3_19
---
> #### 4.2.2 êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
> ![ì‹4_2](./image/04/ì‹4_2.png) 
> ìœ„ ì‹ì€ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ì˜ ì‹ì´ë‹¤.
> 
> ì—¬ê¸°ì„œlogëŠ” ë°‘ì´ eì¸ ìžì—°ë¡œê·¸ì´ë‹¤. YkëŠ” ì‹ ê²½ë§ì˜ ì¶œë ¥. TkëŠ” ì •ë‹µ ë ˆì´ë¸”(ì›-í•« ì¸ì½”ë”©ëœ)ì´ë‹¤.
> 
> ê·¸ëž˜ì„œ ìœ„ ì‹ì€ ì‹¤ì§ˆì ìœ¼ë¡œ ì •ë‹µì¼ ë•Œì˜ ì¶”ì •(Tkê°€ 1ì¼ë•Œì˜ Yk)ì˜ ìžì—°ë¡œê·¸ë¥¼ ê³„ì‚°í•˜ëŠ” ì‹ì´ ëœë‹¤.(ì •ë‹µì´ ì•„ë‹Œ ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ì´ê¸° ë•Œë¬¸ì— ê³±í•˜ì—¬ë„ 0ì´ë˜ê±° ê²°ê³¼ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤.)
> 
> ![ìžì—°ë¡œê·¸](./image/04/ìžì—°ë¡œê·¸.png) 
>
> ìœ„ ê·¸ë¦¼ì€ ìžì—°ë¡œê·¸ì˜ ê·¸ëž˜í”„ë‹¤
> 
> ê·¸ë¦¼ì—ì„œ ë³´ë“¯ xê°€ 1ì¼ ë•Œ yëŠ” 0, xê°€ 0ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ yì˜ ê°’ì€ ì ì  ìž‘ì•„ì§„ë‹¤.
> 
> ìœ„ì—ì„œ ë³¸ ì‹ ë˜í•œ ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì¶œë ¥ì´ ì»¤ì§ˆìˆ˜ë¡ 0ì— ë‹¤ê°€ê°€ë‹¤ í•´ë‹¹ ì¶œë ¥ì´ 1ì¼ë•Œ 0ì´ ëœë‹¤. ë°˜ëŒ€ë¡œ ì •ë‹µì¼ ë•Œì˜ ì¶œë ¥ì´ ìž‘ì•„ì§ˆìˆ˜ë¡ ì˜¤ì°¨ê°€ ì»¤ì§„ë‹¤.
> 
> ```Python
> def cross_entropy_error(y, t)
>     delta = 1e-7
>     return -np.sum(t * np.log(y + delta))
> ```
> ìœ„ ì‹ì€ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ ì½”ë“œë¡œ êµ¬í˜„í•œ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ yì™€ tëŠ” ë„˜íŒŒì´ ë°°ì—´ì´ë‹¤.
> 
> ì½”ë“œ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ë³´ë©´ np.logê³„ì‚°í•  ë•Œ ì•„ì£¼ ìž‘ì€ ê°’ì¸ deltaë¥¼ ë”í–ˆë‹¤. ì´ëŠ” np.log()í•¨ìˆ˜ì— 0ì„ ìž…ë ¥í•˜ë©´ ë§ˆì´ë„ˆìŠ¤ ë¬´í•œëŒ€ë¥¼ ëœ»í•˜ëŠ” -infê°€ ë˜ì–´ ë”ì´ìƒ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
> - ì•„ì£¼ ìž‘ì€ ê°’ì„ ë”í•´ ì ˆëŒ€ë¡œ 0ì´ ë˜ì§€ ì•Šë„ë¡ => ë§ˆì´ë„ˆìŠ¤ ë¬´í•œëŒ€ ë°œìƒ ë°©ì§€
> 
> ì´ì œ ìœ„ì˜ ì½”ë“œë¥¼ í…ŒìŠ¤íŠ¸ í•´ë³¸ë‹¤.
> ```Python
> t = [ 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # ì •ë‹µ 2
> y = [ 0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
> cross_entropy_error(np.array(y), np.array(t)) # 0.51082545709933802
> 
> y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
> cross_entropy_error(np.array(y), np.array(t)) # 2.3025840929945458
> ```
> ì²« ë²ˆì§¸ ì˜ˆì‹œëŠ” ì •ë‹µì¼ ë•Œì˜ ì¶œë ¥ì´ 0.6 ì¸ ê²½ìš°ë¡œ ì´ë•Œì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ëŠ” ì•½ 0.51ì´ë‹¤.
> 
> ë‘ ë²ˆì§¸ ì˜ˆì‹œëŠ” ì •ë‹µì¼ ë•Œì˜ ì¶œë ¥ì´ ë” ë‚®ì€(0.1)ì¸ ê²½ìš°ë¡œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ëŠ” 2.3 ìœ¼ë¡œ ë” í¬ë‹¤.
> 
> ì¦‰ ê²°ê³¼(ì˜¤ì°¨ ê°’)ê°€ ë” ìž‘ì€ ì²« ë²ˆì§¸ ì¶”ì •ì´ ì •ë‹µì¼ ê°€ëŠ¥ì„±ì´ ë” ë†’ë‹¤ê³  íŒë‹¨í•œ ê²ƒì´ë©° ì•žì„  ì˜¤ì°¨ì œê³±í•©ì˜ íŒë‹¨ê³¼ ì¼ì§€í•œë‹¤.
> 
> #### 4.2.3 ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ
> 
> ì§€ê¸ˆ ê¹Œì§€ëŠ” ë°ì´í„° í•˜ë‚˜ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ë§Œ ìƒê°í–ˆì§€ë§Œ ì§€ê¸ˆ ë¶€í„°ëŠ” í›ˆë ¨ ë°ì´í„° ëª¨ë‘ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ í•©ì„ êµ¬í•˜ëŠ” ë°©ë²•ì„ ìƒê°í•´ë³¸ë‹¤.
> 
> ![ì‹4_3](./image/04/ì‹4_3.png)
>
> ìœ„ ì‹ì€ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ì— ì ìš©í•œ ì‹ì´ë‹¤.
> 
> ì´ë•Œ ë°ì´í„°ê°€ Nê°œë¼ë©´ t_(nk)ëŠ” në²ˆì§¸ ë°ì´í„°ì˜ kë²ˆì§¸ ê°’ì„ ì˜ë¯¸í•œë‹¤.(y_(nk)ëŠ” ì‹ ê²½ë§ì˜ ì¶œë ¥, t_(nk)ëŠ” ì •ë‹µ ë ˆì´ë¸”ì´ë‹¤.)
> 
> ì‹ì´ ë³µìž¡í•´ ë³´ì´ì§€ë§Œ ì‚¬ì‹¤ ë‹¨ìˆœížˆ ë°ì´í„° í•˜ë‚˜ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì¸ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ì˜ ì‹ì„ Nê°œì˜ ë°ì´í„°ë¡œ í™•ìž¥í–ˆì„ ë¿ì´ë‹¤.
> 
> ë‹¤ë§Œ ë§ˆì§€ë§‰ì— Nìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì •ê·œí™” í•˜ê³  ìžˆë‹¤. Nìœ¼ë¡œ ë‚˜ëˆ”ìœ¼ë¡œì¨ 'í‰ê·  ì†ì‹¤ í•¨ìˆ˜'ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤.
> 
> ì´ë ‡ê²Œ í‰ê· ì„ êµ¬í•´ ì‚¬ìš©í•˜ë©´ í›ˆë ¨ ë°ì´í„° ê°œìˆ˜ì™€ ê´€ê³„ì—†ì´ ì–¸ì œë“  í†µì¼ëœ ì§€í‘œë¥¼ ì–»ì„ ìˆ˜ ìžˆë‹¤.
> - í›ˆë ¨ ë°ì´í„°ê°€ 1,000ê°œë“  10,000ê°œë“  ìƒê´€ì—†ì´ í‰ê·  ì†ì‹¤ í•¨ìˆ˜ë¥¼ êµ¬í•  ìˆ˜ ìžˆë‹¤.
> 
> í•˜ì§€ë§Œ ëª¨ë“  ë°ì´í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì†ì‹¤í•¨ìˆ˜ì˜ í•©ì„ êµ¬í•˜ê¸°ëŠ” ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦°ë‹¤.
> - ë°ì´í„°ì˜ ì–‘ì´ ë§Žì•„ì§€ê¸° ë•Œë¬¸.(ex. ë¹…ë°ì´í„°)
>
> ì´ëŸ° ê²½ìš° ë°ì´í„°ë¥¼ ì¼ë¶€ë¶„ ì¶”ë ¤ ì „ì²´ì˜ ê·¼ì‚¬ì¹˜ë¡œ ì´ìš©í•  ìˆ˜ ìžˆë‹¤.
> 
> ì‹ ê²½ë§ í•™ìŠµì—ì„œë„ í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° ì¼ë¶€ë§Œ ê³¨ë¼ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤. ì´ ì¼ë¶€ë¥¼ **ë¯¸ë‹ˆë°°ì¹˜**ë¼ê³  í•œë‹¤.
> - 60,000ìž¥ì˜ í›ˆë ¨ ë°ì´í„° ì¤‘ 100ìž¥ì„ ë¬´ìž‘ìœ„ë¡œ ë½‘ì•„ í•´ë‹¹ 100ìž¥ë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤. => ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ
> 
> ```Python
> import sys, os
> sys.path.append(os.pardir) # ë¶€ëª¨ ë””ë ‰í„°ë¦¬ì˜ íŒŒì¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìžˆë„ë¡
> from dataset.mnist import load_mnist
> 
> (x_train, t_train), (x_test, t_test) = load_mnist(normalize=False, one_hot_label=True)
> 
> print(x_train.shape) # (60000, 784)
> print(t_train.shape) # (60000, 10) # ì›-í•« ì¸ì½”ë”©í–ˆìœ¼ë‹ˆê¹Œ (0~9ì¤‘ ì •ë‹µì—ë§Œ 1 ë‚˜ë¨¸ì§€ 0ì´ë‹ˆê¹Œ ë’¤ì— 10ì´ ìƒê¸´ê±°)
> ```
> ìœ„ ì½”ë“œëŠ” ì‹¤ìŠµì„ ì§„í–‰í•˜ê¸° ìœ„í•´ MNISTë°ì´í„° ì…‹ì„ ì½ì–´ì˜¤ëŠ” ì½”ë“œë‹¤ 3ìž¥ì—ì„œë„ ì‚¬ìš©ì„ í–ˆë‹¤.
> 
> ```Python
> train_size = x_train.shape[0]
> batch_size = 10
> batch_mask = np.random.choice(train_size, batch_size)
> x_batch = x_train[batch_mask]
> t_batch = t_train[batch_mask]
> ```
> ìœ„ ì½”ë“œëŠ” batch_sizeì— ìž…ë ¥í•œ ê°’ ë§Œí¼ ë¬´ìž‘ìœ„ë¡œ ë¹¼ë‚´ëŠ” ì½”ë“œë‹¤.
> 
> np.random.choice()ë¡œ ì§€ì •í•œ ë²”ìš°ì˜ ìˆ˜ ì¤‘ ë¬´ìž‘ìœ„ë¡œ ì›í•˜ëŠ” ê°œìˆ˜ë¥¼ íšë“ í•  ìˆ˜ ìžˆë‹¤.
> - np.random.choice(60000, 10) ì€ 0ì´ìƒ60000ë¯¸ë§Œì˜ ìˆ˜ ì¤‘ ë¬´ìž‘ìœ„ë¡œ 10ê°œë¥¼ íšë“í•œë‹¤.
> 
> ì´ì œ ë¬´ìž‘ìœ„ë¡œ ì„ íƒí•œ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë½‘ì•„ë‚¼ ê²ƒì´ë‹¤. ì†ì‹¤ í•¨ìˆ˜ ë˜í•œ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ê³„ì‚°í•œë‹¤.
> 
> #### 4.2.4 (ë°°ì¹˜ìš©)êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ êµ¬í˜„í•˜ê¸°
> ë¯¸ë‹ˆë°°ì¹˜ì™€ ê°™ì€ ë°°ì¹˜ ë°ì´í„°ë¥¼ ì§€ì›í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„  ìœ„ì—ì„œ í•˜ë‚˜ì”© ì²˜ë¦¬í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ êµ¬í˜„í•œ ê²ƒì—ì„œ ì•„ëž˜ì™€ ê°™ì´ ìˆ˜ì •í•˜ë©´ ëœë‹¤.
> ```Python
> def cross_entropy_error(y, t)
>     if y.ndim == 1:
>         t = t.reshape(1, t.size)
>         y = y.reshape(1, y.size)
> 
>     batch_size = y.shape[0]
>     return -np.sum(t * np.log(y + 1e-7))/batch_size
> ```
> ë°ì´í„°ê°€ í•˜ë‚˜ì¸ ê²½ìš°ì™€ ë°ì´í„°ê°€ ë°°ì¹˜ë¡œ ë¬¶ì—¬ ìž…ë ¥ëœ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í•˜ì˜€ë‹¤.
> 
> yëŠ” ì‹ ê²½ë§ì˜ ì¶œë ¥, tëŠ” ì •ë‹µ ë ˆì´ë¸”ì´ë‹¤. yê°€ 1ì°¨ì› ì´ë¼ë©´(ë°ì´í„° í•˜ë‚˜ë‹¹ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ êµ¬í•˜ëŠ” ê²½ìš°) reshapeí•¨ìˆ˜ë¡œ ë°ì´í„°ì˜ í˜•ìƒì„ ë°”ê¿”ì¤€ë‹¤.
> 
> ê·¸ë¦¬ê³  ë°°ì¹˜ì˜ í¬ê¸°ë¥¼ ë‚˜ëˆ  ì •ê·œí™” í•˜ê³  ì´ë¯¸ì§€ 1ìž¥ë‹¹ í‰ê·  êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•œë‹¤.
> 
> ì •ë‹µ ë ˆì´ë¸”ì´ ì›-í•« ì¸ì½”ë”©ì´ ì•„ë‹Œ '2' ë˜ëŠ” '7'ë“±ì˜ ìˆ«ìž ë ˆì´ë¸”ë¡œ ì£¼ì–´ì¡Œì„ ë•Œì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ëŠ” ì•„ëž˜ ì½”ë“œì™€ ê°™ì´ êµ¬í˜„í•  ìˆ˜ ìžˆë‹¤.
> ```Python
> def cross_entropy_error(y, t)
>     if y.ndim == 1:
>         t = t.reshape(1, t.size)
>         y = y.reshape(1, y.size)
> 
>     batch_size = y.shape[0]
>     return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
> ```
> ìœ„ì˜ êµ¬í˜„ì—ì„œëŠ” ì›-í•« ì¸ì½”ë”©ì¼ ë•Œ tê°€ 0ì¸ ì›ì†ŒëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ ë˜í•œ 0ì´ë¯€ë¡œ í•´ë‹¹ ê³„ì‚°ì€ ë¬´ì‹œí•´ë„ ì¢‹ë‹¤ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤.
> - ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì‹ ê²½ë§ì˜ ì¶œë ¥ë§Œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•  ìˆ˜ ìžˆë‹¤ ê·¸ëž˜ì„œ ì›-í•« ì¸ì½”ë”© ì‹œ ```t * np.log(y)```ì˜€ë˜ ë¶€ë¶„ì„ ë ˆì´ë¸” í‘œí˜„ì¼ ë•ŒëŠ” ```np.log(y[np.arange(batch_size), t]```ë¡œ êµ¬í˜„í•œë‹¤.
> 
> ```np.log(y[np.arange(batch_size), t]```ì¤‘ ```np.arange(batch_size)```ëŠ” 0ë¶€í„° batch_size-1ê¹Œì§€ì˜ ë°°ì—´ì„ ìƒì„±í•œë‹¤.
> - batch_sizeê°€ 5ë¼ë©´ [0, 1, 2, 3, 4]ë¼ëŠ” ë„˜íŒŒì´ ë°°ì—´ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.
> 
> tì—ëŠ” ë ˆì´ë¸”ì´ [2, 7, 0, 9, 4]ì˜ í˜•ì‹ìœ¼ë¡œ ì €ìž¥ë˜ì–´ ìžˆë‹¤
> - ì¦‰ ```np.log(y[np.arange(batch_size), t]```ëŠ” ê° ë°ì´í„°ì˜ ì •ë‹µ ë ˆì´ë¸”ì— í•´ë‹¹í•˜ëŠ” ì‹ ê²½ë§ì˜ ì¶œë ¥ì„ ì¶”ì¶œí•œë‹¤.[y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]] ì¸ ë„˜íŒŒì´ ë°°ì—´ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.
> 
--- 
##### 2021_3_23
---
> #### 4.2.5 ì™œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ”ê°€?
> ì •í™•ë„ë¥¼ ì§€í‘œë¡œ í•˜ë©´ ë§¤ê°œë³€ìˆ˜ì˜ ë¯¸ë¶„ì´ ëŒ€ë¶€ë¶„ì˜ ìž¥ì†Œì—ì„œ 0ì´ ë˜ì–´ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.
> 
> ![ì‹œê·¸ëª¨ì´ë“œ_ê³„ë‹¨](./image/03/ì‹œê·¸ëª¨ì´ë“œ_ê³„ë‹¨.png)
> 
> ê³„ë‹¨ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ìœ„ ê·¸ë¦¼ì—ì„œ ì§ì„ ìœ¼ë¡œ êµ¬ì„±ëœ ê·¸ëž˜í”„ì™€ ê°™ì´ ëŒ€ë¶€ë¶„ì˜ êµ¬ê°„ì—ì„œ 0ì´ë‹¤. ê·¸ë¡œì¸í•´ ê³„ë‹¨ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì§€í‘œë¡œ ì‚¼ëŠ” ê²Œ ì•„ë¬´ ì˜ë¯¸ê°€ ì—†ë‹¤.
> - ë§¤ê°œë³€ìˆ˜ì˜ ìž‘ì€ ë³€í™”ê°€ ì£¼ëŠ” íŒŒìž¥ì„ ê³„ë‹¨ í•¨ìˆ˜ê°€ ë§ì‚´í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì—ëŠ” ì•„ë¬´ë³€í™”ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.
> 
> ê³„ë‹¨ í•¨ìˆ˜ê°€ í•œìˆœê°„ë§Œ ë³€í™”ë¥¼ ì¼ì´í‚¤ëŠ” ê²ƒì— ë¹„í•´ ìœ„ ê·¸ë¦¼ì—ì„œ ê³¡ì„ ìœ¼ë¡œ êµ¬ì„±ëœ ê·¸ëž˜í”„ì¸ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ì¶œë ¥(ì„¸ë¡œì¶•)ì´ ì—°ì†ì ìœ¼ë¡œ ë³€í™”í•˜ê³  ê³¡ì„ ì˜ ê¸°ìš¸ê¸° ë˜í•œ ì—°ì†ì ìœ¼ë¡œ ë³€í™”í•œë‹¤.
> 
> ë˜í•œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ì–´ëŠ ìž¥ì†Œë¼ê³  0ì´ ë˜ì§€ ì•Šìœ¼ë©° ì´ ì„±ì§ˆì€ ì‹ ê²½ë§ í•™ìŠµì—ì„œ ì¤‘ìš”í•œ ì„±ì§ˆížˆë©° ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì‹ ê²½ë§ì´ ì˜¬ë°”ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.
> 
> #### 4.3 ìˆ˜ì¹˜ ë¯¸ë¶„
> ê²½ì‚¬ë²•ì—ì„œëŠ” ê¸°ìš¸ê¸°(ê²½ì‚¬) ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ë°©í–¥ì„ ì •í•œë‹¤.
> 
> #### 4.3.1 ë¯¸ë¶„
> 
> ![ì‹4_4](./image/04/ì‹4_4.png)
>
> ìœ„ ì‹ì€ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤. ì¢Œë³€ì€ f(x)ì˜ xì— ëŒ€í•œ ë¯¸ë¶„(xì— ëŒ€í•œf(x)ì˜ ë³€í™”ëŸ‰)ì„ ë‚˜íƒ€ë‚´ëŠ” ê¸°í˜¸ë‹¤.
> - ì¦‰ xì˜ 'ìž‘ì€ ë³€í™”'ê°€ í•¨ìˆ˜ f(x)ë¥¼ ì–¼ë§ˆë‚˜ ë³€í™”ì‹œí‚¤ëŠëƒë¥¼ ì˜ë¯¸í•¨
> 
> - ì´ ë•Œ ì‹œê°„ì˜ ìž‘ì€ ë³€í™”ëŠ” ì‹œê°„ì„ ëœ»í•˜ëŠ” hë¥¼ í•œì—†ì´ 0ì— ê°€ê¹ê²Œ í•œë‹¤ëŠ” ì˜ë¯¸ë¡œ lim_(h->0)ì„ ì‚¬ìš©
> 
> ```Python
> def numerical_diff(f, x):
>     h = 10e-50
>     return (f(x + h) - f(x)) / h
> ```
> ìœ„ì˜ ì‹ê³¼ ê°™ì´ hì— ìž‘ì€ ê°’ì„ ëŒ€ìž…í•´ ê³„ì‚°í•  ìˆ˜ë„ ìžˆë‹¤.
> 
> í•˜ì§€ë§Œ ìœ„ ì‹ì€ 2ê°€ì§€ ë¬¸ì œê°€ ìžˆë‹¤
> 
>> ì²« ë²ˆì§¸ë¡œ 'h = 10e-50' ëŠ” ì†Œìˆ˜ì  ì•„ëž˜ 0ì´49ê°œ ë¼ëŠ” ëœ»ì´ë‹¤ 0.00..1 í•˜ì§€ë§Œ ì´ ë°©ì‹ì€ ë°˜ì˜¬ë¦¼ ì˜¤ì°¨ë¥¼ ë°œìƒì‹œí‚¨ë‹¤.
>> 
>> ë„ˆë¬´ ìž‘ì€ ê°’ì´ê¸° ë•Œë¬¸ì— np.float32(1e-50)ì˜ ê°’ì´ 0.0ì´ë˜ì–´ ì˜¬ë°”ë¡œ í‘œí˜„í•  ìˆ˜ê°€ ì—†ë‹¤.
>> - ë„ˆë¬´ ìž‘ì€ ê°’ì„ ì´ìš©í•˜ë©´ ì»´í“¨í„°ë¡œ ê³„ì‚°í•˜ëŠ”ë° ë¬¸ì œê°€ ìƒê¸´ë‹¤.
>> 
>> ì´ë¥¼ ìœ„í•´ 10^(-4)ì •ë„ì˜ ê°’ì„ ì‚¬ìš©í•˜ëŠ”ê²ƒì´ ê°€ìž¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤ê³  ì•Œë ¤ì ¸ ìžˆë‹¤.
>> 
>> ë‘ ë²ˆì§¸ ë¬¸ì œëŠ” í•¨ìˆ˜ fì˜ ì°¨ë¶„(ìž„ì˜ ë‘ ì ì—ì„œ í•¨ìˆ˜ ê°’ë“¤ì˜ ì°¨ì´)ê´€ë ¨ ë¬¸ì œë‹¤.
>>  
>> ì§„ì •í•œ ë¯¸ë¶„ì€ xìœ„ì¹˜ì˜ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°(ì ‘ì„ )ì— í•´ë‹¹í•˜ì§€ë§Œ ì§€ê¸ˆ êµ¬í˜„ì—ì„œ ë¯¸ë¶„ì€ (x+h)ì™€ xì‚¬ì´ì˜ ê¸°ìš¸ê¸°ì— í•´ë‹¹í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
>> 
>> ì´ ì°¨ì´ëŠ” hë¥¼ ë¬´í•œížˆ 0ìœ¼ë¡œ ì¢ížˆëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•´ ìƒê¸°ëŠ” í•œê³„ë‹¤.
>> 
>> ![ì§„ì •í•œ_ë¯¸ë¶„_ê·¼ì‚¬ë¡œ_êµ¬í•œ_ë¯¸ë¶„](./image/04/ì§„ì •í•œ_ë¯¸ë¶„_ê·¼ì‚¬ë¡œ_êµ¬í•œ_ë¯¸ë¶„.png)
>>
>> ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ìˆ˜ì¹˜ ë¯¸ë¶„ì—ëŠ” ì˜¤ì°¨ê°€ í¬í•¨ëœë‹¤. ê·¸ ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´(x+h)ì™€ (x-h)ì¼ ë•Œì˜ í•¨ìˆ˜ fì˜ ì°¨ë¶„ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ì“°ê¸°ë„ í•œë‹¤.
>> 
>> ì´ ì°¨ë¶„ì€ xë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê·¸ ì „í›„ì˜ ì°¨ë¶„ì„ ê³„ì‚°í•œë‹¤ëŠ” ì˜ë¯¸ì—ì„œ ì¤‘ì‹¬ ì°¨ë¶„ í˜¹ì€ ì¤‘ì•™ ì°¨ë¶„ì´ë¼ í•œë‹¤
>> -  (x + h)ì™€ xì˜ ì°¨ë¶„ì€ ì „ë°© ì°¨ë¶„ì´ë¼í•œë‹¤.
>
> ìœ„ ë‘ê°€ì§€ ë¬¸ì œì ì„ ê°œì„ í•´ ì•„ëž˜ì™€ ê°™ì´ ìˆ˜ì¹˜ë¯¸ë¶„ì„ ë‹¤ì‹œ êµ¬í˜„í•œë‹¤.
> ```Python
> def numerical_diff(f, x):
>     h = 1e-4
>     return (f(x + h) - f(x - h)) / (2*h)
> ```
> 
> #### 4.3.2 ìˆ˜ì¹˜ ë¯¸ë¶„ì˜ ì˜ˆ
> ì•žì˜ ìˆ˜ì¹˜ ë¯¸ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•´ë³¸ë‹¤.
> 
> ![ì‹4_5](./image/04/ì‹4_5.png)
> 
> ìœ„ ì‹ì„ ì•„ëž˜ì™€ ê°™ì´ íŒŒì´ì¬ì„ ì´ìš©í•´ êµ¬í˜„í•˜ê³  ê·¸ëž˜í”„ë¥¼ ê·¸ë ¤ë³¸ë‹¤.
> ```Python
> import numpy as np
> import matplotlib.pylab as plt
> def function_1(x):
>     return 0.01*x**2 + 0.1*x
> 
> x = np.arange(0.0, 20.0, 0.1)
> y = function_1(x)
> plt.xlabel("x")
> plt.ylabel("f(x)")
> plt.plot(x, y)
> plt.show()
> ```
> 
> ![ì‹4_5_ê·¸ëž˜í”„](./image/04/ì‹4_5_ê·¸ëž˜í”„.png)
> 
> ìœ„ì™€ ê°™ì´ ê·¸ëž˜í”„ê°€ ê·¸ë ¤ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìžˆë‹¤.
> 
> ìœ„ì—ì„œ ì‚¬ìš©í•œ ì‹ì˜ í•´ì„ì  í•´ëŠ” 0.02x+0.1ì´ë‹¤ ë˜í•œ ìœ„ ì½”ë“œë¥¼ ì´ìš©í•´ x=5, x=10ì¼ë•Œ ê³„ì‚°í•´ ë³¸ë‹¤ë©´ 0.1999999999990898, 0.2999999999986347ì´ë‹¤. 
> - ì§„ì •í•œ ë¯¸ë¶„ì€ í•´ì„ì  í•´ë¥¼ ì´ìš©í•´ ê³„ì‚°í•˜ë©´ ì°¨ë¡€ë¡œ 0.2, 0.3ì´ë‹¤ ì˜¤ì°¨ê°€ ë§¤ìš° ìž‘ìŒì„ ì•Œ ìˆ˜ ìžˆë‹¤.
> 
> ì´ì œ ì•žì—ì„œ êµ¬í•œ ìˆ˜ì¹˜ ë¯¸ë¶„ ê°’ì„ ê¸°ìš¸ê¸°ë¡œ í•˜ëŠ” ì§ì„ ì„ ê·¸ë¦°ë‹¤ ê²°ê³¼ëŠ” ì•„ëž˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.
>
> ![x_5_x_10_ì—ì„œ_ì ‘ì„ ](./image/04/x_5_x_10_ì—ì„œ_ì ‘ì„ .png)
> 
> ìœ„ ê·¸ë¦¼ì„ í†µí•´ í•¨ìˆ˜ì˜ ì ‘ì„ ì•  í•´ë‹¹í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìžˆë‹¤.
> 
> #### 4.3.3 íŽ¸ë¯¸ë¶„
> ![ì‹4_6](./image/04/ì‹4_6.png)
> 
> ìœ„ ì‹ì€ ì¸ìˆ˜ë“¤ì˜ ì œê³± í•©ì„ ê³„ì‚°í•˜ëŠ” ë‹¨ìˆœí•œ ì‹ì´ì§€ë§Œ ì•žì˜ ì˜ˆì™€ ë‹¬ë¦¬ ë³€ìˆ˜ê°€ 2ê°œë¼ëŠ” ì ì— ì£¼ì˜í•´ì•¼í•œë‹¤.
> 
> ì½”ë“œë¡œ êµ¬í˜„í•œë‹¤ë©´ ì•„ëž˜ì™€ ê°™ë‹¤.
> ```Python
> def function_2(x):
>     return x[0]**2 + x[1]**2 # ë˜ëŠ” np.sum(x**2)
> ```
> ì¸ìˆ˜ xëŠ” ë„˜íŒŒì´ ë°°ì—´ì´ë¼ê³  ê°€ì •í•œë‹¤. ìœ„ ì½”ë“œëŠ” ë„˜íŒŒì´ ë°°ì—´ì˜ ê° ì›ì†Œë¥¼ ì œê³±í•˜ê³  ê·¸ í•©ì„ êµ¬í•˜ëŠ” ê°„ë‹¨í•œ êµ¬í˜„ì´ë‹¤. ê·¸ëž˜í”„ë¡œ ê·¸ë¦°ë‹¤ë©´ ì•„ëž˜ì™€ ê°™ë‹¤.
> 
> ![4_8](./image/04/4_8.png)
> 
> ì´ì œ ë¯¸ë¶„ì„ í•´ì•¼í•œë‹¤ í•˜ì§€ë§Œ ë³€ìˆ˜ê°€ 2ê°œì´ê¸° ë•Œë¬¸ì— ì–´ëŠ ë³€ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„ì¸ì§€ êµ¬ë³„í•´ì•¼í•œë‹¤.
> - ì´ì²˜ëŸ¼ ë³€ìˆ˜ê°€ ì—¬ëŸ¿ì¸ í•¨ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„ì„ íŽ¸ë¯¸ë¶„ì´ë¼ í•œë‹¤.
> 
>> ![ex_1](./image/04/ex_1.png)
>> 
>> ```Python
>> def function_tmp1(x0):
>>     return x0*x0 + 4.0**2.0
>> 
>> numerical_diff(function_tmp1, 3.0) # 6.00000000000378
>> ````
>> 
>> ![ex_2](./image/04/ex_2.png)
>> 
>> ```Python
>> def function_tmp2(x1):
>>     return 3.0**2.0 + x1*x1
>> 
>> numerical_diff(function_tmp2, 4.0) # 7.999999999999119
>> ````
>> 
>> ìœ„ ë¬¸ì œë“¤ì€ ë³€ìˆ˜ê°€ í•˜ë‚˜ì¸ í•¨ìˆ˜ë¥¼ ì €ìœ¼ì´í•˜ê³  ê·¸í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ëŠ” í˜•ì±„ë¡œ êµ¬í˜„í•˜ì—¬ í’€ì—ˆë‹¤.
>> - ë¬¸ì œ 1ì—ì„œ x_1=4ë¡œ ê³ ì •ëœ ìƒˆë¡œìš´ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ë³€ìˆ˜ê°€ x_0í•˜ë‚˜ ë¿ì¸ í•¨ìˆ˜ì— ëŒ€í•´ ìˆ˜ì¹˜ ë¯¸ë¶„ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì˜€ë‹¤.
>> 
>> ê·¸ë¦¬ê³  ê°ê°ì˜ ê²°ê³¼ëŠ” í•´ì„ì  ë¯¸ë¶„ì˜ ê²°ê³¼ì™€ ê±°ì˜ ê°™ì€ê²ƒì„ í™•ì¸í•  ìˆ˜ ìžˆë‹¤.
>> 
>> ì´ì™€ê°™ì´ íŽ¸ë¯¸ë¶„ì€ ë³€ìˆ˜ê°€ í•˜ë‚˜ì¸ ë¯¸ë¶„ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ íŠ¹ì • ìž¥ì†Œë¦ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œë‹¤.
>> - ë‹¨ ì—¬ëŸ¬ ë³€ìˆ˜ ì¤‘ ëª©í‘œ ë³€ìˆ˜ í•˜ë‚˜ì— ì´ˆì ì„ ë§žì¶”ê³  ë‹¤ë¥¸ ë³€ìˆ˜ëŠ” ê°’ì„ ê³ ì •í•œë‹¤
>> 
>> ì•žì˜ ì˜ˆì—ì„œ ëª©í‘œ ë³€ìˆ˜ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ë¥¼ íŠ¹ì • ê°’ì— ê³ ì •ì‹œí‚¤ê¸° ìœ„í•´ ìƒˆë¡œìš´ í•¨ìˆ˜ë¥¼ ì •ì˜í–ˆë‹¤ ê·¸ë¦¬ê³  ê·¸ ìƒˆë¡œ ì •ì˜í•œ í•¨ìˆ˜ì— ëŒ€í•´ ê·¸ë™ì•ˆ ì‚¬ìš©í•œ ìˆ˜ì¹˜ ë¯¸ë¶„ í•¨ìˆ˜ë¥¼ ì ìš©í•´ íŽ¸ë¯¸ë¶„ì„ êµ¬í•œ ê²ƒì´ë‹¤.
>
---
##### 2021_3_24
---
> #### 4.4 ê¸°ìš¸ê¸°
> ëª¨ë“ ë³€ìˆ˜ì˜ íŽ¸ë¯¸ë¶„ì„ ë²¡í„°ë¡œ ì •ë¦¬í•œ ê²ƒì„ ê¸°ìš¸ê¸° ë¼ê³  í•˜ë©° ì•„ëž˜ì™€ ê°™ì´ êµ¬í˜„í•  ìˆ˜ ìžˆë‹¤.
> ```Python
> def numerical_gradient(f, x)
>     h = 1e-4 # 0.0001
>     grad = np.zeros_like(x) # xì™€ í˜•ìƒì´ ê°™ì€ ë°°ì—´ì„ ìƒì„±
>     
>     for iddx in range(x.size):
>         tmp_val = x[idx]
>         
>         # f(x+h)ê³„ì‚°
>         x[idx] = tmp_val + h
>         fxh1 = f(x)
>         
>         # f(x-h)ê³„ì‚°
>         x[idx] = tmp_val - h
>         fxh2 = f(x)
>
>         grad[idx] = (fxh1 - fxh2) / (2*h)
>         x[idx] = tmp_val # ê°’ ë³µì›
>     
>     return grad
> ```
> numerical_gradient(f, x)í•¨ìˆ˜ì˜ êµ¬í˜„ì€ ë³µìž¡í•˜ê²Œ ë³´ì´ì§€ë§Œ ë™ìž‘ë°©ì‹ì€ ë³€ìˆ˜ê°€ í•˜ë‚˜ì¼ ë•Œì˜ ìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ê±°ì˜ ìœ ì‚¬í•˜ë‹¤.
> - np.zeros_like(x)ëŠ” xì™€ í˜•ìƒì´ ê°™ê³  ê·¸ ì›ì†Œê°€ ëª¨ë‘ 0ì¸ ë°°ì—´ì„ ìƒì„±í•œë‹¤.
> 
> numerical_gradient(f, x) í•¨ìˆ˜ì˜ ì¸ìˆ˜ì¸ fëŠ” í•¨ìˆ˜ì´ê³  xëŠ” ë„˜íŒŒì´ ë°°ì—´ì´ë‹¤
> 
> ë„˜íŒŒì´ ë°°ì—´ xì˜ ê° ì›ì†Œì— ëŒ€í•´ ìˆ˜ì¹˜ ë¯¸ë¶„ì„ êµ¬í•œë‹¤.
> - ìœ„ì—ì„œ ìž‘ì„±í•œ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ì‹¤ì œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•´ ë³¸ë‹¤(ì„¸ ì  (3,4), (0,2), (3,0))
> ```Python
> numerical_gradient(function_2, np.array([3.0, 4.0])) # array([ 6., 8.]) *
> numerical_gradient(function_2, np.array([0.0, 2.0])) # array([ 0., 4.])
> numerical_gradient(function_2, np.array([3.0, 0.0])) # array([ 6., 0.])
> ```
> ìœ„ì™€ê°™ì´ ê° ì ì—ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìžˆë‹¤.
> 
> ![4_9](./image/04/4_9.png)
>
> ê¸°ìš¸ê¸° ê·¸ë¦¼ì€ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ë°©í–¥ì„ ê°€ì§„ ë²¡í„°(í™”ì‚´í‘œ)ë¡œ ê·¸ë ¤ì§„ë‹¤. ìœ„ ê·¸ë¦¼ì„ ë³´ë©´ ê¸°ìš¸ê¸°ëŠ” í•¨ìˆ˜ì˜ 'ê°€ìž¥ ë‚®ì€ ìž¥ì†Œ(ìµœì†Ÿê°’)'ë¥¼ ê°€ë¦¬í‚¤ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.
> - í™”ì‚´í‘œë“¤ì´ í•œ ì ì„ í–¥í•˜ê³  ìžˆë‹¤.
> 
> ë˜í•œ 'ê°€ìž¥ ë‚®ì€ ê³³'ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ í™”ì‚´í‘œì˜ í¬ê¸°ê°€ ì»¤ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìžˆë‹¤.
> 
> ê¸°ìš¸ê¸°ëŠ” ê°€ìž¥ ë‚®ì€ ìž¥ì†Œë¥¼ ê°€ë¦¬í‚¤ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë°˜ë“œì‹œ ê·¸ë ‡ë‹¤ê³  í•  ìˆ˜ëŠ” ì—†ë‹¤ê³  í•œë‹¤.
> 
> ì‚¬ì‹¤ ê¸°ìš¸ê¸°ëŠ” ê° ì§€ë„˜ì—ì„œ ë‚®ì•„ì§€ëŠ” ë°©í–¥ì„ ê°€ë¦¬í‚¨ë‹¤ ì •í™•ížˆ ë§í•˜ë©´ 
> 
> **ê¸°ìš¸ê¸°ê°€ ê°€ë¦¬í‚¤ëŠ” ìª½ì€ ê° ìž¥ì†Œì—ì„œ í•¨ìˆ˜ì˜ ì¶œë ¥ ê°’ì„ ê°€ìž¥ í¬ê²Œ ì¤„ì´ëŠ” ë°©í–¥**ì´ë‹¤.
> 
> #### 4.4.1 ê²½ì‚¬ë²•(ê²½ì‚¬ í•˜ê°•ë²•)
> ê¸°ê³„í•™ìŠµ ë¬¸ì œ ëŒ€ë¶€ë¶„ì€ í•™ìŠµ ë‹¨ê³„ì—ì„œ ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì°¾ì•„ë‚¸ë‹¤.
> 
> ì‹ ê²½ë§ ë˜í•œ ìµœì ì˜ ë§¤ê°œë³€ìˆ˜(ê°€ì¤‘ì¹˜, íŽ¸í–¥)ì„ ì°¾ì•„í–í•œë‹¤.
> - ìµœì ì´ëž€ ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì†Ÿê°’ì´ ë  ë•Œì˜ ë§¤ê°œë³€ìˆ˜ ê°’ì´ë‹¤.
> 
> - ì¼ë°˜ì ì¸ ë¬¸ì œì˜ ì†ì‹¤ í•¨ìˆ˜ëŠ” ë§¤ìš° ë³µìž¡í•˜ë‹¤. ë§¤ê°œë³€ìˆ˜ ê³µê°„ì´ ê´‘ëŒ€í•´ ì–´ë””ê°€ ìµœì†Ÿê°’ì´ ë˜ëŠ” ê³³ì¸ì§€ ì§ìž‘í•˜ê¸° íž˜ë“¤ê¸° ë•Œë¬¸
> 
> ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ê¸°ìš¸ê¸°ë¥¼ ìž˜ ì´ìš©í•´ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’(ë˜ëŠ” ê°€ëŠ¥í•œ ê°€ìž¥ ìž‘ì€ ê°’)ì„ ì°¾ìœ¼ë ¤ëŠ” ê²ƒì´ **ê²½ì‚¬ë²•**ì´ë‹¤.
> 
> ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ ê° ì§€ì ì—ì„œ í•œìˆ˜ì˜ ê°’ì„ ë‚®ì¶”ëŠ” ë°©ì•ˆì„ ì œì‹œí•˜ëŠ” ì§€í‘œê°€ ê¸°ìš¸ê¸°ë¼ëŠ” ì ì´ë‹¤.
> - ê¸°ìš¸ê¸°ê°€ ê°€ë¦¬í‚¤ëŠ” ê³³ì— ì •ë§ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì´ ìžˆëŠ”ì§€(ê·¸ìª½ì´ ì •ë§ë¡œ ë‚˜ì•„ê°ˆ ë°©í–¥ì¸ì§€) ë³´ìž¥í•  ìˆ˜ ì—†ë‹¤.
> 
> - ì‹¤ì œë¡œ ë³µìž¡í•œ í•¨ìˆ˜ì—ì„œëŠ” ê¸°ìš¸ê¸°ê°€ ê°€ë¦¬í‚¤ëŠ” ë°©í–¥ì— ìµœì†Ÿê°’ì´ ì—†ëŠ” ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì´ë¼ê³  í•œë‹¤.
> 
>> í•¨ìˆ˜ê°€ ê·¹ì†Ÿê°’, ìµœì†Ÿê°’, ë˜ [**ì•ˆìž¥ì **](https://ko.wikipedia.org/wiki/%EC%95%88%EC%9E%A5%EC%A0%90)ì´ ë˜ëŠ” ìž¥ì†Œì—ì„œëŠ” ê¸°ìš¸ê¸°ê°€ 0ì´ë‹¤. ê·¹ì†Ÿê°’ì€ êµ­ì†Œì ì¸ ìµœì†Ÿê°’. ì¦‰ í•œì •ëœ ë²”ìœ„ì—ì„œì˜ ìµœì†Ÿê°’ì¸ ì ì´ë‹¤.
>> 
>> ì•ˆìž¥ì ì€ ë³´ëŠ” ë°©í–¥ì— ë”°ë¼ ê·¹ëŒ“ê°’ê³¼ ê·¹ì†Ÿê°’ì´ ë˜ëŠ” ì ì´ë‹¤. ê²½ì‚¬ë²•ì€ ê¸°ìš¸ê¸°ê°€ 0ì¸ ìž¥ì†Œë¥¼ ì°¾ì§€ë§Œ ê·¸ê²ƒì´ ë°˜ë“œì‹œ ìµœì†Ÿê°’ì´ë¼ê³  í•  ìˆ˜ëŠ” ì—†ë‹¤.(ê·¹ì†Ÿê°’ì´ë‚˜ ì•ˆìž¥ì ì¼ ê°€ëŠ¥ì„±ì´ ìžˆë‹¤.)
>> 
>> ë˜, ë³µìž¡í•˜ê³  ì°Œê·¸ëŸ¬ì§„ ëª¨ì–‘ì˜ í•¨ìˆ˜(ëŒ€ë¶€ë¶„)ë¼ë©´ í‰í‰í•œ ê³³ìœ¼ë¡œ íŒŒê³ ë“¤ë©° **ê³ ì›**(plateeu)ì´ë¼ í•˜ëŠ” í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠëŠ” ì •ì²´ê¸°ì— ë¹ ì§ˆ ìˆ˜ ìžˆë‹¤.
>
> ê¸°ìš¸ì–´ì§„ ë°©í–¥ì´ ê¼­ ìµœì†Ÿê°’ì„ ê°€ë¦¬í‚¤ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ ê·¸ ë°©í–¥ìœ¼ë¡œ ê°€ì•¼ í•¨ìˆ˜ì˜ ê°’ì„ ì¤„ì¼ ìˆ˜ ìžˆë‹¤.
> - ê·¸ëž˜ì„œ ìµœì†Ÿê°’ì´ ë˜ëŠ” ìž¥ì†Œë¥¼ ì°¾ëŠ” ë¬¸ì œ(ì•„ë‹ˆë©´ ê°€ëŠ¥í•œ í•œ ìž‘ì€ ê°’ì´ ë˜ëŠ” ìž¥ì†Œë¥¼ ì°¾ëŠ” ë¬¸ì œ)ì—ì„œëŠ” ê¸°ìš¸ê¸° ì •ë³´ë¥¼ ë‹¨ì„œë¡œ ë‚˜ì•„ê°ˆ ë°©í–¥ì„ ì •í•´ì•¼ í•œë‹¤.
> 
> ì´ë¥¼ ìœ„í•´ **ê²½ì‚¬ë²•**ì„ ì‚¬ìš©í•œë‹¤.
>> ê²½ì‚¬ë²•ì€ í˜„ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ì¼ì • ê±°ë¦¬ë§Œí¼ ì´ë™í•œë‹¤.
>> 
>> ê·¸ ë‹¤ìŒ ì´ë™í•œ ê³³ì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ê³  ë‹¤ì‹œ ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°€ëŠ” ê²ƒì„ ë°˜ë³µí•œë‹¤.
>> 
>> ì´ ê³¼ì •ì„ í†µí•´ í•¨ìˆ˜ì˜ ê°’ì„ ì ì°¨ ì¤„ì—¬ë‚˜ê°„ë‹¤.
>>
> ìœ„ ê³¼ì •ê³¼ ê°™ì´ í•¨ìˆ˜ì˜ ê°’ì„ ì ì°¨ ì¤„ì´ëŠ” ê²ƒì´ **ê²½ì‚¬ë²•**ì´ë©° ê²½ì‚¬ë²•ì€ ê¸°ê³„í•™ìŠµì„ ìµœì í™”í•˜ëŠ” ë° í”ížˆ ì“°ëŠ” ë°©ë²•ì´ë‹¤.
> - íŠ¹ížˆ ì‹ ê²½ë§ í•™ìŠµì—ëŠ” ê²½ì‚¬ë²•ì„ ë§Žì´ ì‚¬ìš©í•œë‹¤.
> 
>> ê²½ì‚¬ë²•ì€ ìµœì†Ÿê°’ì„ ì°¾ëŠ”ê²½ìš° **ê²½ì‚¬ í•˜ê°•ë²•** ìµœëŒ“ê°’ì„ ì°¾ëŠ”ê²½ìš° **ê²½ì‚¬ ìƒìŠ¹ë²•**ì´ë¼í•œë‹¤.
>> 
>> ì†ì‹¤ í•¨ìˆ˜ì˜ ë¶€í˜¸ë¥¼ ë°˜ì²œì‹œí‚¤ë©´ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ë¬¸ì œì™€ ìµœëŒ“ê°’ì„ ì°¾ëŠ” ë¬¸ì œëŠ” ê°™ë‹¤.
>> 
>> ì¼ë°˜ì ìœ¼ë¡œ ì‹ ê²½ë§(ë”¥ëŸ¬ë‹) ë¶„ì•¼ì—ì„œì˜ ê²½ì‚¬ë²•ì€ 'ê²½ì‚¬ í•˜ê°•ë²•'ìœ¼ë¡œ ë“±ìž¥í•˜ëŠ” ê²½ìš°ê°€ ë§Žë‹¤.
>> 
> ![ì‹4_7](./image/04/ì‹4_7.png)
>
> ìœ„ ì‹ì€ ê²½ì‚¬ë²•ì˜ ì‹ì´ë‹¤, ì—íƒ€ë¥¼ ì‹ ê²½ë§ í•™ìŠµì—ì„œëŠ” **í•™ìŠµë¥ **ì´ë¼í•œë‹¤.
> - í•œë²ˆì˜ í•™ìŠµìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ í•™ìŠµí•´ì•¼ í• ì§€(ë§¤ê°œë³€ìˆ˜ ê°’ì„ ì–¼ë§ˆë‚˜ ê°±ì‹ í•´ì•¼ëŠ”ì§€) ì •í•˜ëŠ” ê²ƒì´ í•™ìŠµë¥ ì´ë‹¤.
> 
> ìœ„ ì‹ì€ 1íšŒì— í•´ë‹¹í•˜ëŠ” ê°±ì‹ ì´ê³  ì´ ë‹¨ê³„ë¥¼ ë°˜ë³µí•œë‹¤, ì¦‰ ìœ„ ì‹ì²˜ëŸ¼ ë³€ìˆ˜ì˜ ê°’ì„ ìƒì‹ í•˜ëŠ” ë‹¨ê³„ë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ë©´ì„œ ì„œì„œížˆ í•¨ìˆ˜ì˜ ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ë‹¤.
> - ì—¬ê¸°ì„  2ê°œì˜ ë³€ìˆ˜ì¸ ê²½ìš°ì§€ë§Œ ë³€ìˆ˜ì˜ ìˆ˜ê°€ ëŠ˜ì–´ë„ ê°™ì€ ì‹(ê° ë³€ìˆ˜ì˜ íŽ¸ë¯¸ë¶„ ê°’)ìœ¼ë¡œ ê°±ì‹ í•˜ê²Œ ëœë‹¤.
> 
> ë˜í•œ í•™ìŠµë¥ ì€ 0.01ì´ë‚˜ 0.001ë“± ë¯¸ë¦¬ íŠ¹ì • ê°’ìœ¼ë¡œ ì •í•´ë‘ì–´ì•¼ í•œë‹¤.
> 
> ì¼ë°˜ì ìœ¼ë¡œ ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ìž‘ìœ¼ë©´ 'ì¢‹ì€ ìž¥ì†Œ'ë¥¼ ì°¾ì•„ê°ˆ ìˆ˜ ì—†ê¸°ë•Œë¬¸ì— ì‹ ê²½ë§ í•™ìŠµì—ì„  ì´ í•™ìŠµë¥  ê°’ì„ ë³€ê²½í•˜ë©° ì˜¬ë°”ë¥´ê²Œ í•™ìŠµí•˜ê³  ìžˆëŠ”ì§€ í™•ì¸í•˜ë©° ì§„í–‰í•œë‹¤.
> 
> ê²½ì‚¬ í•˜ê°•ë²•ì€ ì•„ëž˜ì™€ ê°™ì´ êµ¬í˜„í•  ìˆ˜ ìžˆë‹¤.
> ```Python
> def gradient_descent(f, init_x, lr=0.01, step_num=100):
>     x = init_x
>     
>     for i in range(step_num):
>         grad = numerical_gradient(f,x)
>         x -= lr * grad
>     return x
> ```
> ì¸ìˆ˜fëŠ” ìµœì í™”í•˜ë ¤ëŠ” í•¨ìˆ˜, init_xëŠ” ì´ˆê¹ƒê°’, lrì€ [learning rate](https://bioinformaticsandme.tistory.com/130)ë¥¼ ì˜ë¯¸í•˜ëŠ” í•™ìŠµë¥ , step_numì€ ê²½ì‚¬ë²•ì— ë”°ë¥¸ ë°˜ë³µ íšŸìˆ˜ë¥¼ ëœ»í•œë‹¤.
> 
> í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ëŠ” numerical_gradient(f,x)ë¡œ êµ¬í•˜ê³  ê·¸ ê¸°ìš¸ê¸°ì— í•™ìŠµë¥ ì„ ê³±í•œ ê°’ìœ¼ë¡œ ê°±ì‹ í•˜ëŠ” ì²˜ë¦¬ë¥¼ step_numë²ˆ ë°˜ë³µí•œë‹¤.
> 
> ìœ„ ì½”ë“œë¥¼ ì´ìš©í•´ í•¨ìˆ˜ì˜ ê·¹ì†Ÿê°’ì„ êµ¬í•  ìˆ˜ ìžˆê³  ìž˜í•˜ë©´ ìµœì†Ÿê°’ì„ êµ¬í•  ìˆ˜ë„ ìžˆë‹¤.
> 
> ![ì‹4_7ë¬¸ì œ](./image/04/ì‹4_7ë¬¸ì œ.png)
> 
> ```Python
> def function_2(x):
>     return x[0]**2 + x[1]**2
> 
> init_x = np.array([-3.0, 4.0])
> def gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)
> # array([ -6.11110793e-10, 8.14814391e-10])
> ```
> ì—¬ê¸°ì„  ì´ˆê¸°ê°’ì„ (-3.0, 4.0)ë¡œ ì„¤ì •í•œ í›„ ê²½ì‚¬ë²•ì„ ì‚¬ìš©í•´ ìµœì†Ÿê°’ì„ íƒìƒ‰í•œë‹¤.
> 
> ìµœì¢… ê²°ê³¼ëŠ” ê±°ì˜ (0, 0)ì— ê°€ê°€ìš´ ê²°ê³¼ë‹¤. ì‹¤ì œë¡œ ì§„í–‰í•­ ìµœì†Ÿê°’ì€ (0, 0)ì´ë¯€ë¡œ ê²½ì‚¬ë²•ì€ ê±°ì˜ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì€ ê²ƒì´ë‹¤.
> 
> ê²½ì‚¬ë²•ì„ ì‚¬ìš©í•œ ì´ ê°±ì‹  ê³¼ì •ì„ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ëž˜ ê·¸ë¦¼ ì²˜ëŸ¼ ëœë‹¤.
> 
> ![4_10](./image/04/4_10.png)
>
> ê°’ì´ ê°€ìž¥ ë‚®ì€ ìž¥ì†Œì¸ ì›ì ì— ì ì°¨ ê°€ê¹Œì›Œ ì§€ê³  ìžˆë‹¤.
> 
>> í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ë‚®ì€ ê²½ìš°ëŠ” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ì—†ë‹¤.
>> 
>> í° ê²½ìš°ëŠ” í° ê°’ìœ¼ë¡œ ë°œì‚°í•˜ê³  ìž‘ì€ ê²½ìš°ëŠ” ê±°ì˜ ê°±ì‹ ë˜ì§€ ì•Šì€ì²´ ì¢…ë£Œí•˜ê²Œ ëœë‹¤.
>
---
#### 3/27
---
> #### 4.4.2 ì‹ ê²½ë§ì—ì„œ ê¸°ìš¸ê¸°
> ì‹ ê²½ë§ í•™ìŠµì—ì„œë„ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•´ì•¼í•œë‹¤.
> - ì—¬ê¸°ì„œ ë§í•˜ëŠ” ê¸°ìš¸ê¸°ëŠ” ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë‹¤.
>> ì˜ˆì‹œë¡œ í˜•ìƒì´ 2X3, ê°€ì¤‘ì¹˜ê°€ W, ì†ì‹¤ í•¨ìˆ˜ê°€ Lì¸ ì‹ ê²½ë§ì„ ìƒê°í•´ ë³¼ ë•Œ ðœ•L/ðœ•Wë¡œ ë‚˜íƒœë‚  ìˆ˜ ìžˆìœ¼ë©° ìˆ˜ì‹ìœ¼ë¡œëŠ” ì•„ëž˜ì™€ ê°™ë‹¤.
> 
> ![ì‹4_8](./image/04/ì‹4_8.png)
> 
> ðœ•L/ðœ•Wì˜ ê° ì›ì†ŒëŠ” ê°ê°ì˜ ì›ì†Œì— ê´€í•œ íŽ¸ë¯¸ë¶„ì´ë‹¤. 
> - ì˜ˆë¥¼ ë“¤ì–´ 1í–‰ 1ë²ˆì§¸ ì›ì†Œì˜ w_11ì„ ë³€ê²½ í•˜ì˜€ì„ ë•Œ ì†ì‹¤ í•¨ìˆ˜ Lì´ ì–¼ë§ˆë‚˜ ë³€í™”í•˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤.
> 
> - ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€ ðœ•L/ðœ•Wì˜ í˜•ìƒì´ Wì™€ ê°™ë‹¤ëŠ” ì ì´ë‹¤.(ì‹¤ì œë¡œ ìœ„ ì‹ì—ì„œ Wì™€ ðœ•L/ðœ•Wì˜ í˜•ìƒì€ ëª¨ë‘ 2X3ì´ë‹¤.)
> 
> ì•„ëž˜ëŠ” ê°„ë‹¨í•œ ì‹ ê²½ë§ì„ ì˜ˆë¡œë“¤ì–´ ì‹¤ì œë¡œ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í•œ ê²ƒì´ë‹¤.
> ```Python
> import sys, os
> sys.path.append(os.pardir)
> import numpy as np
> from common.functions import softmax, cross_entropy_error
> from common.gradient import numerical_gradient
> 
> class simpleNet:
>     def __init__(self):
>         self.W = np.random.randn(2,3) # ì •ê·œë¶„í¬ë¡œ ì´ˆê¸°í™”
> 
>     def predict(self, x):
>         return np.dot(x, self.W)
> 
>     def loss(self, x, t):
>         z = self.predict(x)
>         y = softmax(z)
>         loss = cross_entropy_error(y, t)
> 
>         return loss
> ```
> ìœ„ ì½”ë“œì—ì„  ì˜ˆì œ í´ë”ì— ìžˆëŠ” common/functions.pyì— ì •ì˜í•œ softmaxì™€ cross_entropy_error ë©”ì„œë“œë¥¼ ì‚¬ìš©í•œë‹¤.
> 
> ë˜í•œ common/gradient.pyì— ì •ì˜í•œ numerical_gradient ë©”ì„œë“œë„ ì´ìš©í•œë‹¤.
> 
> simpleNet í´ëž˜ìŠ¤ëŠ” í˜•ìƒì´ 2X3ì¸ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ í•˜ë‚˜ë¥¼ [ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜](https://wikidocs.net/1744)ë¡œ ê°–ëŠ”ë‹¤.
> 
> ë©”ì„œë“œëŠ” 2ê°œë‹¤.
> - predict(x) ëŠ” ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.
> 
> - loss(x,t) ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ êµ¬í•œë‹¤.
> 
> ì¸ìˆ˜ xëŠ” ìž…ë ¥ ë°ì´í„°, tëŠ” ì •ë‹µ ë ˆì´ë¸”ì´ë‹¤.
> ```Python
> x = np.array([0.6, 0.9]) # ìž…ë ¥ ë°ì´í„°
> t = np.array([0, 0, 1]) # ì •ë‹µ ë ˆì´ë¸”
> 
> net = simpleNet()
> 
> f = lambda w: net.loss(x, t)
> dW = numerical_gradient(f, net.W) # ê¸°ìš¸ê¸°
> 
> print(net.W) # ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜
> print(dW) 
> p = net.predict(x) # ì˜ˆì¸¡
> print(p)
> print(np.argmax(p)) # ìµœëŒ“ê°’ì˜ ì¸ë±ìŠ¤
> print(net.loss(x,t)) # ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’
> 
> ''' ì‹¤í–‰ê²°ê³¼
> [[ 1.318572   -2.09024228 -1.05120929] => print(net.W)
> [ 0.40789182  1.11277335 -0.46496514]]
> [[ 0.01129628  0.36169778 -0.37299406] => print(dW)
>  [ 0.01694443  0.54254667 -0.55949109]]
> [-1.61301654  1.8533186   1.38748507] => print(p)
> 1 => print(np.argmax(p))
> 0.9719536539973302 => print(net.loss(x,t))
> '''
> ```
> ìœ„ì™€ê°™ì´ ìž„ì˜ì˜ ê°’ì„ ì´ìš©í•´ simpleNetí´ëž˜ìŠ¤ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ì•˜ë‹¤.
> 
> ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ”ê²ƒì€ ì§€ê¸ˆê¹Œì§€ ì²˜ëŸ¼ numerical_gradient(f,x)ë¥¼ ì‚¬ìš©í•´ êµ¬ë¼ë©´ ëœë‹¤.
> - ì—¬ê¸°ì„œ ì •ì˜í•œ f(W)í•¨ìˆ˜ì˜ ì¸ìˆ˜ WëŠ” ë”ë¯¸ë¡œ ë§Œë“  ê²ƒì´ë‹¤. numerical_gradient(f,x)ë‚´ë¶€ì—ì„œ f(x)ë¥¼ ì‹¤í–‰í•˜ëŠ”ë° ì¼ê´€ì„±ì„ ìœ„í•´ f(W)ë¥¼ ì •ì˜í•œ ê²ƒ.
> 
> ì‹ ê²½ë§ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œ ë‹¤ìŒì—ëŠ” ê²½ì‚¬ë²•ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤.
> 
---
3/30
---
> #### 4.5 í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„í•˜ê¸°
> ì‹ ê²½ë§ í•™ìŠµì˜ ì ˆì°¨ëŠ” ì•„ëž˜ì™€ ê°™ë‹¤.
> 
> **ì „ì œ**
>> ì‹ ê²½ë§ì—ëŠ” ì ì‘ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì™€ íŽ¸í–¥ì´ ìžˆê³ , ì´ ê°€ì¤‘ì¹˜ì™€ íŽ¸í–¥ì„ í›ˆë ¨ ë°ì´í„°ì— ì ì‘í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ 'í•™ìŠµ'ì´ë¼ í•œë‹¤. 
>>
>> ì‹ ê²½ë§ í•™ìŠµì€ ì•„ëž˜ì™€ ê°™ì´ 4ë‹¨ê³„ë¡œ ìˆ˜í–‰í•œë‹¤.
>>
>> **1ë‹¨ê³„-ë¯¸ë‹ˆë°°ì¹˜**
>>> í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ìž‘ìœ„ë¡œ ê°€ì ¸ì˜¨ë‹¤. ì´ë ‡ê²Œ ì„ ë³„í•œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¼ í•˜ë©° ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œë‹¤.
>>
>> **2ë‹¨ê³„-ê¸°ìš¸ê¸° ì‚°ì¶œ**
>>> ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œê°€. ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ìž¥ ìž‘ê²Œ í•˜ëŠ” ë°©í–¥ì„ ì œì‹œí•œë‹¤.
>>
>> **3ë‹¨ê³„-ë§¤ê°œë³€ìˆ˜ ê°±ì‹ **
>>> ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì•„ì£¼ ì¡°ê¸ˆ ê°±ì‹ í•œë‹¤.
>>
>> **4ë‹¨ê³„-ë°˜ë³µ**
>>> 1~3ë‹¨ê³„ë¥¼ ë°˜ë³µí•œë‹¤.
>>
> ìœ„ ë‹¨ê³„ê°€ ì‹ ê²½ë§ í•™ìŠµì´ ì´ë¤„ì§€ëŠ” ìˆœì„œë‹¤. ì´ëŠ” ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ëŠ” ë°©ë²•ì´ë©°, ì´ë•Œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë¬´ìž‘ìœ„ë¡œ ì„ ì •í•˜ê¸° ë•Œë¬¸ì— [**í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•**](https://go-hard.tistory.com/11)ì´ë¼ ë¶€ë¥¸ë‹¤.
> - 'í™•ë¥ ì ìœ¼ë¡œ ë¬´ìž‘ìœ„ë¡œ ê³¨ë¼ë‚¸ ë°ì´í„°'ì— ëŒ€í•´ ìˆ˜í–‰í•˜ëŠ” ê²½ì‚¬ í•˜ê°•ë²•ì´ë¼ëŠ” ì˜ë¯¸
> 
> -  ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ í”„ë ˆìž„ì›Œí¬ì—ì„  í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent)ì˜ ì˜ì–´ ë¨¸ë¦¬ ê¸€ìžë¥¼ ë”°ì„œ SGDë¼ëŠ” í•¨ìˆ˜ë¡œ í•´ë‹¹ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ê³  ìžˆë‹¤.
>
> #### 4.5.1 2ì¸µ ì‹ ê²½ë§ í´ëž˜ìŠ¤ êµ¬í˜„í•˜ê¸°
> ì†ê¸€ì”¨ ìˆ«ìžë¥¼ í•™ìŠµí•˜ëŠ” ì‹ ê²½ë§ì„ êµ¬í˜„í•´ ë³¸ë‹¤. 2ì¸µ ì‹ ê²½ë§(ì€ë‹‰ì¸µì´ 1ê°œì¸ ë„¤íŠ¸ì›Œí¬)ì„ ëŒ€ìƒìœ¼ë¡œ MNISTë°ì´í„° ì…‹ì„ ì‚¬ìš©í•´ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.
> 
> ì²˜ìŒì—” 2ì¸µ ì‹ ê²½ë§ì„ í•˜ë‚˜ì˜ í´ëž˜ìŠ¤ë¡œ êµ¬í˜„í•˜ëŠ” ê²ƒ ë¶€í„° ì‹œìž‘í•œë‹¤.
> ```Python
> import sys, os
> sys.path.append(os.pardir)  # ë¶€ëª¨ ë””ë ‰í„°ë¦¬ì˜ íŒŒì¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìžˆë„ë¡ ì„¤ì •
> from common.functions import *
> from common.gradient import numerical_gradient
> import numpy as np
> 
> class TwoLayerNet:
> 
>     def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
>         # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
>         self.params = {}
>         self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
>         self.params['b1'] = np.zeros(hidden_size)
>         self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
>         self.params['b2'] = np.zeros(output_size)
> 
>     def predict(self, x):
>         W1, W2 = self.params['W1'], self.params['W2']
>         b1, b2 = self.params['b1'], self.params['b2']
>     
>         a1 = np.dot(x, W1) + b1
>         z1 = sigmoid(a1)
>         a2 = np.dot(z1, W2) + b2
>         y = softmax(a2)
>         
>         return y
>         
>     # x : ìž…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
>     def loss(self, x, t):
>         y = self.predict(x)
>         
>         return cross_entropy_error(y, t)
>     
>     def accuracy(self, x, t):
>         y = self.predict(x)
>         y = np.argmax(y, axis=1)
>         t = np.argmax(t, axis=1)
>         
>         accuracy = np.sum(y == t) / float(x.shape[0])
>         return accuracy
>         
>     # x : ìž…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
>     def numerical_gradient(self, x, t):
>         loss_W = lambda W: self.loss(x, t)
>         
>         grads = {}
>         grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
>         grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
>         grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
>         grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
>         
>         return grads
>         
>     def gradient(self, x, t):
>         W1, W2 = self.params['W1'], self.params['W2']
>         b1, b2 = self.params['b1'], self.params['b2']
>         grads = {}
>         
>         batch_num = x.shape[0]
>         
>         # forward
>         a1 = np.dot(x, W1) + b1
>         z1 = sigmoid(a1)
>         a2 = np.dot(z1, W2) + b2
>         y = softmax(a2)
>         
>         # backward
>         dy = (y - t) / batch_num
>         grads['W2'] = np.dot(z1.T, dy)
>         grads['b2'] = np.sum(dy, axis=0)
>         
>         da1 = np.dot(dy, W2.T)
>         dz1 = sigmoid_grad(a1) * da1
>         grads['W1'] = np.dot(x.T, dz1)
>         grads['b1'] = np.sum(dz1, axis=0)
> 
>         return grads
> ```
> ì½”ë“œëŠ” ìœ„ì™€ ê°™ë‹¤. ê°ê°ì˜ ë©”ì„œë“œ ì„¤ëª…ì€ ì•„ëž˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.
> 
> ![TowLayerNet_ì„¤ëª…](./image/04/TowLayerNet_ì„¤ëª….png)
> 
> TowLayerNet í´ëž˜ìŠ¤ëŠ” ë”•ì…”ë„ˆë¦¬ì¸ paramsì™€ gradsë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ê°–ëŠ”ë‹¤.
> 
> params ë³€ìˆ˜ì—” ê°€ì¤‘ì¹˜ ë§¤ê°œ ë³€ìˆ˜ê°€ ì €ìž¥ëœë‹¤.
> - ì˜ˆ : params[â€˜W1â€™]ì€ 1ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜, Params[â€˜b1â€™]ì€ 1ë²ˆì§¸ ì¸µì˜ íŽ¸í–¥ì´ ì €ìž¥ëœë‹¤.
> ```Python
> net = TowLayerNet(input_size = 784, hidden_size = 100, output_size = 10)
> net.params['W1'].shape # (784, 100)
> net.params['b1'].shape # (100,)
> net.params['W2'].shape # (100, 10)
> net.params['b2'].shape # (10,)
> ```
> ìœ„ì™€ ê°™ì´ paramsë³€ìˆ˜ì—” ì‹ ê²½ë§ì— í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ê°€ ëª¨ë‘ ì €ìž¥ëœë‹¤.
> 
> params ë³€ìˆ˜ì— ì €ìž¥ëœ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ê°€ ì˜ˆì¸¡ ì²˜ë¦¬(ìˆœë°©í–¥ ì²˜ë¦¬)ì—ì„œ ì‚¬ìš©ëœë‹¤.
>
> ```Python
> x = np.random.rand(100, 784) # ë”ë¯¸ ìž…ë ¥ ë°ì´í„°(100ìž¥ ë¶„ëŸ‰)
> y = net.predict(x)
> ```
> ì˜ˆì¸¡ì²˜ë¦¬ëŠ” ìœ„ì™€ ê°™ì´ ì‹¤í–‰í•  ìˆ˜ ìžˆë‹¤.
> 
> grads ë³€ìˆ˜ì—” params ë³€ìˆ˜ì— ëŒ€ì‘í•˜ëŠ” ê° ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ê°€ ì €ìž¥ëœë‹¤.
> - ì˜ˆ : ì•„ëž˜ì™€ ê°™ì´ numerical_gradient() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ë©´ gradsë³€ìˆ˜ì— ê¸°ìš¸ê¸° ì •ë³´ê°€ ì €ìž¥ëœë‹¤.
>> ```Python
>> x = np.random.rand(100, 784) # ë”ë¯¸ ìž…ë ¥ ë°ì´í„°(100ìž¥ ë¶„ëŸ‰)
>> t = np.random.rand(100, 10) # ë”ë¯¸ ì •ë‹µ ë ˆì´ë¸”(100ìž¥ ë¶„ëŸ‰)
>> 
>> grads = net.numerical_gradient(x, t)
>> 
>> grads['W1'].shape # (784, 100) 
>> grads['b1'].shape # (100,)
>> grads['W2'].shape # (100, 10)
>> grads['b2'].shape # (10,)
>> ```
> ì´ì–´ì„œ TowLayerNetë©”ì„œë“œë¥¼ ì‚´íŽ´ë³¸ë‹¤.
> 
> __init__(self.input_size, hidden_size, output_size) ë©”ì„œë“œ
>> í´ëž˜ìŠ¤ë¥¼ ì´ˆê¸°í™” í•œë‹¤.
>> 
>> ì´ˆê¸°í™” ë©”ì„œë“œëŠ” TowLayerNetì„ ìƒì„±í•  ë•Œ ë¶ˆë¦¬ëŠ” ë©”ì„œë“œë‹¤.
>> 
>> ì¸ìˆ˜ëŠ” ìˆœì„œëŒ€ë¡œ ìž…ë ¥ì¸µì˜ ë‰´ëŸ° ìˆ˜, ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜, ì¶œë ¥ ì¸µì˜ ë‰´ëŸ° ìˆ˜
>> - ì˜ˆ : ì†ê¸€ì”¨ ìˆ«ìž ì¸ì‹ì—ì„œëŠ” í¬ê¸°ê°€ 28X28ì¸ ìž…ë ¥ ì´ë¯¸ì§€ê°€ ì´ 784ê°œ ì´ê³  ì¶œë ¥ì€ 10ê°œê°€ ëœë‹¤ ë”°ë¼ì„œ input_size=784, output_ size=10ìœ¼ë¡œ ì§€ì •í•˜ê³  ì€ë‹‰ì¸µì˜ê°œìˆ˜ì¸ hidden_sizeëŠ” ì ë‹¹í•œ ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤.
>> 
>> í•´ë‹¹ ì´ˆê¸°í™” ë©”ì„œë“œì—ì„œëŠ” ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë„ ì´ˆê¸°í™”í•œë‹¤. 
>> - ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ì´ˆê¹ƒê°’ì„ ë¬´ì—‡ìœ¼ë¡œ ì„¤ì •í•˜ëƒê°€ ì‹ ê²½ë§ í•™ìŠµì˜ ì„±ê³µì„ ì¢Œìš°í•˜ê¸°ë„ í•œë‹¤.
>
> predict(self.x, t)ì™€ accuracy(self.x, t)ì˜ êµ¬í˜„ì€ ì•žì„œ ë³¸ ì‹ ê²½ë§ì˜ ì¶”ë¡  ì²˜ë¦¬ì™€ ê±°ì˜ ë¹„ìŠ·í•˜ë‹¤[3.6.2ì ˆ](https://github.com/minchan5224/TIL/blob/main/Python/Deep_Learning/Deep_01_03.md#362-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EC%B6%94%EB%A1%A0-%EC%B2%98%EB%A6%AC)
> 
> loss(self.s, t)ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê³„ì‚°í•˜ëŠ” ë©”ì„œë“œë‹¤.
> - ì´ ë©”ì„œë“œëŠ” predict()ì˜ ê²°ê³¼ì™€ ì •ë‹µ ë ˆì´ë¸”ì„ ë°”íƒ•ìœ¼ë¡œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤íŒŒë¥¼ êµ¬í•˜ë„ë¡ êµ¬í˜„ë˜ì—ˆë‹¤.
> 
> numerical_gradient(self.x, t)ë©”ì„œë“œëŠ” ê° ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤.
> - ìˆ˜ì¹˜ ë¯¸ë¶„ ë°©ì‹ìœ¼ë¡œ ê° ë§¤ê°œë³€ìˆ˜ì˜ ì†ì‹¤ í•¨ìˆ˜ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°.
> 
> gradient(self.x, t)ëŠ” ë‹¤ìŒìž¥ì—ì„œ êµ¬í˜„.
> - ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì‚¬ìš©í•´ ê¸°ìš¸ê¸°ë¥¼ íš¨ìœ¨ì ì´ê³  ë¹ ë¥´ê²Œ ê³„ì‚°í•œë‹¤.
> 
> #### 4.5.2 ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ êµ¬í˜„í•˜ê¸°
