## [밑바닥부터 시작하는 딥러닝]
---
### 목차
- []()
---
#### CHAPTER6_학습 관련 기술들
---
> #### 6.1 매개변수 갱신
> 확률적 경사 하강법(SGD)
> - 매개변수의 기울기(미분)을 구해 기울어진 방향으로 매개변수 값을 갱신하는 일을 반복하여 점점 최적의 값에 다가가는 방법
> 
> 지금까지는 SGD를 이용했지만 이제 SGD의 단점을 알아보고 SGD와 다른 최적화 기법을 학습한다.
> 
> #### 6.1.1 모험가 이야기
> 최적화 해야하는 상황을 모험가에 비유한다.
> 
>> 세상에서 가장 깊고 낮은 골짜기인 '깊은 곳'을 찾아 가려한다 하지만 2가지의 제약이 있다. '지도를 보지 않는것' 과 '눈가리개를 쓰는 것' 이다.
>> 
>> 이러한 제약 조건이 있을 때 이 모험가는 어떻게 '깊은 곳'을 찾아 갈 수 있을까 어떨게 걸음을 옮겨야 효율적으로 '깊은 곳'을 찾아 낼 수 있을 것인가?
>
> 위 상황과 우리의 상황은 비슷하다 또한 위의 상황에서 가장 중요한 단서가 되는 것은 '기울기'다.
> 
> 주위의 시각적인 정보를 습득 할 수 없지만 서있는 땅의 기울기는 알 수 있다.
> 
> 이때 지금 서있는 장소에서 가장 크게 기울어진 방향으로 가는 것이 SGD의 전략이다.
> 
> #### 6.1.2 확률적 경사 하강법(SGD)
> SGD는 수식으로는 아래와 같다. 
>
> ![6_1식](./image/06/6_1식.png)
> 
> 실제로 학습률은 0.01이나 0.001과 같은 값을 미리 정해서 사용하며 '←' 는 우변의 값을 좌변의 값으로 갱신한다는 뜻이다.
> 
> 위 식에서 보듯 SGD는 기울어진 방향으로 일정 거리만 가겠다는 단순한 방법이다.
> 
> 아래는 파이썬을 이용해 SGD를 구현한 코드다.
> 
> ```Python
> class SGD:
>     def __init__(self, lr = 0.01):
>         self.lr = lr
>     
>     def update(self, params, grads):
>         for key in params.keys():
>             params[key] -= self.lr * grads[key]
> ```
> 초기화 때 받는 인수인 lr은 learning rate(학습률)을 뜻한다. 이 학습률은 인스턴스 변수로 유지한다.
> 
> update(params, grads)메서드는 SDG과정에서 반복해서 호출된다. 인스턴스 변수인 params, grads는 딕셔너리 변수다.
> 
> params['W1'], grads['W1']과 같이 각각의 가중치 매개변수와 기울기를 저장하고 있다.
> 
> SGD클래스를 이용하면 신경망 매개 변수의 진행을 아래와 같이 수행할 수 있다.
> ```Python
> network = TwoLayerNet(...)
> optimizer = SGD() #SGD 사용
> for i in range(10000):
>     ...
>     x_batch, t_batch = get_mini_batch(...) #미니배치
>     grads = network.gradient(x_batch, t_batch)
>     params = network.parmas
>     optimizer.update(params, grads)
>     ...
> ```
> optimizer는 '최적화를 행하는 자'라는 뜻의 단어이며 위 코드에선 SGD가 그 역할을 한다.
> 
> 매개변수 갱신은 optimizer가 수행하므로 optimizer에 매개변수 기울기 정보만 넘겨주면 된다.
> 
> 이와같이 최적화를 담당하는 클래스를 분리해 구현하면 기능을 모듈화 하기 좋다.
> - 선언할때만 ```optimizer = SGD()```에서 ```optimizer = 사용할 클래스``` 의 방식으로 바꿔 사용이 가능하다. 
> 
> #### 6.1.3 SGD의 단점
> 다음 함수의 최솟값을 구하는 문제를 푼다.
> 
> ![비등방성함수](./image/06/비등방성함수.png)
> 
>  SGD를 이용한다면 결과는 아래와 같이 심하게 굽어진(지그재그가 심한)모습을 보인다.
>
> ![SGD_img](./image/06/SGD_img.png) 
>
> SGD함수의 가장 큰 단점은 위 그림과 같이 비등방성 함수(방향에 따라 성질, 여기선 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이기 때문이다.
> 
> 이럴 때에는 SGD같이 무작정 기울어진 방향으로 진행하는 단순한 방식말고 다른 방식이 필요해진다.
> - 모멘텀, AdaGrad, Adam등
> 
> #### 6.1.4 모멘텀
> 모멘텀은 운동량을 뜻하는 단어로 물리와 관계가 있으며 식으로 표현하면 아래와 같다.
> 
> ![모멘텀](./image/06/모멘텀.png)
> 
> 위 식은 기울기 방향으로 힘을 받아 물체가 가속된다는 물리 법칙을 나타내며 공이 그릇의 바닷을 구르는 듯한 움직임을 보여준다.
> 
> ![모멘텀_2](./image/06/모멘텀_2.png)
> 
> 또한 위 식에서 𝛼v항은 물제가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 한다(𝛼는 0.9등의 값으로 설정한다). 물리에서 지면의 마찰이나 공기의 저항에 해당한다.
> 
> 아래 코드는 모멘텀의 구현이다. 
> ```Python
> class Momentum:
>     def __init__(self, lr=0.01, momentum=0.9):
>         self.lr = lr
>         self.momentum = momentum
>         self.v = None # 물체의 속도
>         
>     def update(self, params, grads):
>         if self.v is None:
>             self.v = {} # 초기화 시엔 None로 초기화 한 뒤 처음 호출될 때 매개 변수와 같은 구조의 데이터를 딕셔너리 변수로 저장한다.
>             for key, val in params.items():                                
>                 self.v[key] = np.zeros_like(val)
>                 
>         for key in params.keys():
>             self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] 
>             params[key] += self.v[key]
> ```
> 이제 모멘텀을 이용해 풀면 아래와 같은 결과가 나온다.
> 
> ![모멘텀_img](./image/06/모멘텀_img.png)
> 
> 그림에서 보듯 모멘텀의 갱신 경로는 SGD에 비해 지그재그 정도가 덜하며 그릇 바닥을 구르듯 움직이는 것을 볼 수 있다.
> 
> 이는 x축의 힘은 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속하기 때문이다. 
> 
> 그 반대로 y축의 힘은 크지만 위아래로 번갈아 받아 상충하여 y축 방향의 속도는 안정적이지 않다.
> 
> 전체적으로는 SGD보다 x축 방향으로 빠르게 다가가 지그재그 움직임이 줄어든다.
> 
> #### 6.1.5 AdaGrad
> 신경망 학습에선 학습률 값이 중요하다. 값이 너무 작으면 학습 시간이 매우 길어지고 너무 크면 학습이 제대로 이루어지지 않는다.
> 
> 학습률을 정하는 효과적 기술로 **학습률 감소**가 있다. 
> 
> 학습을 진행하면서 학습률을 점차 줄여가는 방법이다
> - 처음엔 크게 학습하다 조금식 작게 학습하는 방식이며 실제 신경망 학습에 자주 사용된다.
> 
> 학습률을 서서히 낮추는 가장 간단한 방법은 매개변수'전체'의 학습률 값을 일괄적으로 낮추는 것이며 이를 더욱 발전시킨 것이 AdaGrad이다.
> - AdaGrad 는 '각각의' 매개변수에 '맞춤형' 값을 만들어 준다.
> 
> AdaGrad 는 개별 매개변수에 적응적으로 학습률을 조정하면서 학습을 진행한다. AdaGrad의 갱신 방법은 수식으로는 아래와 같다.
> 
> ![AdaGrad_식](./image/06/AdaGrad_식.png)
> 
> 1/√ℎ 곱하는 것은 매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률이 낮아진다는 뜻
> - 학습률 감소가 매개 변수의 원소마다 다르게 적용됨
> 
>> AdaGrad는 과거의 기울기를 제곱하여 계속 더해간다. 그로인해 학습을 진행할 수록 갱신 강도가 약해지며 무한히 학습한다면 어느 순간 갱신량이 0이 되어 갱신되지 않게 된다.
>> 
>> 이러한 문제를 개선한 기법으로 RMSProp이라는 방법이 있으며 이 방법은 과저의 모든 기울기를 균일하게 더해가는 것이 아닌 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영하는 방법이다.
>> 
>> 또한 이를 **지수이동평균**이라 하여 과거 기울기 반영 규모를 기하급수적으로 감소 시킨다.
>
> 이제 AdaGrad의 코드를 살펴본다.
> ```Python
> class AdaGrad:
>   def __init__(self, lr=0.01):
>       self.lr = lr
>       self.h = None
>       
>   def update(self, params, grads):
>       if self.h is None:
>           self.h = {}
>           for key, val in params.items():
>               self.h[key] = np.zeros_like(val)
>           
>       for key in params.keys():
>           self.h[key] += grads[key] * grads[key]
>           params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
> ```
> 위 코드에서 주의할 점은 마지막 줄에서 1e-7이라는 작은 값을 더하는 부분이다.
> 
> 이 작은 값은 self.h[key]에 0이 담겨잇다 해도 0으로 나누는 사태를 막아준다. 대부분의 딥러닝 프레임워크에서는 이 값도 인수로 설정할 수 있다.
> 
> AdaGrad를 이용해 풀면 아래와 같은 결과가 나온다.
> 
> ![AdaGrad_img](./image/06/AdaGrad_img.png)
> 
> 위 그림을 보면 최솟값을 향해 효율적으로 움직이는 것을 확인 할 수 있다. y축 방향은 기울기가 커서 처음엔 크게 움직이지만 큰 움직임에 비례해 갱신 정도도 큰 폭으로 작아지도록 조정된다.
> 
> 그래서 y축 방향으로 갱신의 강도가 빠르게 약해지고 크그재그 움직임이 줄어든다.
> 
> #### 6.1.6 Adam
> 간단히 말하면 모멘텀과 AdaGrad 두 기법을 융합한 것이라 생각하면 된다.
> 
> 이론은 다소 복잡하지만 직관적으로는 모멘텀과 AdaGrad를 융합한 듯한 방법이다. 매개변수 공간을 효율적으로 탐색해줄 것으로 기대해도 좋을 것이라 한다.
> 
> 또한 하이퍼파라미터의 '편향 보정'이 진행된다는 점도 Adam의 특징이며 책에서는 더 깊게 파고들지는 않는다.
> 
> Adam을 이용해 문제를 풀면 아래 그림과 같다.
> 
> ![Adam_img](./image/06/Adam_img.png)
> 
> Adam의 갱신 과정도 그릇 바닥을 구르듯 움직인다. 모멘텀과 비슷하지만 좌우 흔들림이 적다.
> - 이는 학습의 갱신 강도를 적응적으로 조정해서 얻는 혜택이다.
> 
> #### 6.1.7 어느 갱신 방법을 이용할 것인가?
> 지금까지 확인한 갱신 방법 4개를 비교해본다.
> 
> ![6_1_7_1](./image/06/6_1_7_1.png)
> 
> 위 그림과 같이 사용한 기법에 따라 갱신 경로가 다르다.
> - 그림만 봤을땐 AdaGrad가 가장 효율적으로 보이지만 풀어야할 문제의 종류와 (학습률 등의)하이퍼파라미터를 어떻게 설정하느냐에 따라 결과가 달라진다.
> 
> 각각의 상황을 고려해 선택하는 것이 좋다.
> 
> #### 6.1.8 MINST 데이터셋으로 본 갱신 방법 비교
> 손글씨 숫자 인식을 대상으로 지금까지 설명한 네 기법을 비교해 본다.
> 
> 각 층이 100개의 뉴런으로 구성된 5층 신경망에서 ReLU를 활성화 함수로 사용해 측정한 것이다.
> 
> ![MNIST_갱신방법비교](./image/06/MNIST_갱신방법비교.png)
> 
> SGD의 학습 진도가 가장 느리며 나머지의 진도는 비슷하다(AdaGrad가 조금 더 빨라보인다)
> 
> 이 실험 또한 하이퍼파라미터인 학습률과 신경망의 구조(층, 깊이 등)에 따라 결과가 달라진다는 점이다.
> 
> 일반적으론 SGD보다 다른 3기법이 빠르게 학습하며 때론 최종 정확도도 높게 나타난다.
> 
> #### 2.2 단순한 논리 회로
> #### 2.2.1 AND 게이트
