### 멀티캠퍼스 인공지능 자연어처리[NLP]기반 기업 데이터 분석.
- 9주차 3일 07/07
---
## 선형 대수
> ### 벡터
> 
> 벡터는 크기와 방향을 가지고 있는 개념을 표현한 것이다.
> 
> 단어나 문장을 텍스트 그대로 사용할 수 없기 때문에 단어를 표현하거나 문장을 표현할 때 벡터를 사용한다.
> 
> - 벡터의 덧셈
>> 두 벡터간 덧셈은 각 벡터의 원소들간 값을 더하여 계산한다.
>
> - 벡터의 뺄셈
>> 마찬가지로 두 벡터간 뺄셈은 각 벡터의 원소들간 값을 빼서 계산한다.
>
> - 벡터의 스칼라 곱셈/나눗셈
>> 벡터에 스칼라 곱셈과 나눗셈을 적용하면 스칼라 값을 각 원소에 곱하거나 나누어 계산한다.
>
> - 벡터의 norm
>> 벡터의 노름(norm)은 벡터의 크기/길이를 의미한다.
>
> - 벡터의 내적(dot product)
>> 두 벡터의 내적은 두 벡터의 개별 성분의 곱의 합이다.
>
> - 벡터의 직교
>> 두 벡터 x와 y가 내적이 0이면 서로 직교한다.
>
> ### 행렬
> 
> 행렬은 2차원 숫자 배열이다. 행렬은 벡터의 묶음으로 생각할 수 있다.
> 
> - 대각 행렬(diagonal matrix)
>> 행과 열의 크기가 같은 정방행렬 비대각 요소의 값이 모두 0이고 대각 요소만 0이 아닌 값을 가진 행렬
>
> - 직교 행렬 (Orthogonal matrix)
>> 행벡터와 열벡터가 직교를 이루는 행렬
> 
> - 단위 행렬 (Identify matrix)
>> 행과 열의 크기가 같은 정방행렬의 비대각 요소의 값이 모두 0이고 대각 요소만 1의 값을 가진 행렬
>
> - 전치 행렬 (Transposed matrix)
>> 전치 행렬은 행과 열이 바뀐 행렬이다.
> 
> - 역행렬 (Inverse matrix)
>> 역행렬은 임의행렬 X에 X-1을 곱했을 때 단위 행렬 I 가 되는 행렬 X^-1
>
> - [선형 변환(Linear Transformation)](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EB%B3%80%ED%99%98)
>> 벡터에 행렬을 곱한다. 
>
> - 고유 벡터(Eigenvector)
>> 행렬 A를 곱하더라도 방향이 변하지 않고 그 크기만 변한다.
>
> ### 확률
> - 기대값 (Expected Value)
>> 어떤 확률을 가진 사건을 무한히 반복했을 경우 얻을 수 있는 값의 평균으로서 기대할 수 있는 값
>
> - 분산 (Variance)
>> 분포가 평균으로부터 얼마나 산포되어 있는 정도
>>
>> 편차의 제곱의 기댓값
>
> - 표준편차 (Standard Deviation)
>> 분산에 제곱근을 붙인 값
>
> - 공분산(Covariance)
>> 두 개의 확률변수의 관계를 보여주는 값
>>
>> 확률변수 X와 Y에 대해 X가 변할 대 Y가 변하는 정도를 나타내는 값
>
> ### 공분산 행렬(Covariance Matrix)
> 
> #### 공분산 행렬의 의미
> - 데이터의 의미 :  각 feature의 움직임이 얼마나 유사한가
> 
> - 수학적 의미: 선형변환
> 
> 고유벡터는 그 행렬이 벡터에 작용하는 주축(principal axis)의 방향을 나타내므로 공분산 행렬의 고유벡터는 데이터가 어떤 방향으로 분산되어 있는지를 나타내 준다.
> 
> #### 공분산 행렬의 의미 - 데이터 구조
>> 어떤 행렬 X
>> - 행은 sample, 열은 feature를 의미
>> 
>> - 열(feature)의 평균 값을 0으로 조정 (=각 feature의 값에 평균을 뺀 상태
>> 
>> - 행렬 X의 공분산 행렬 계산
>> 
>> 데이터의 구조적 의미 : 각 feature의 변동이 얼마나 닮았나.
>> - 공분산 행렬의 dot(X_1, X_1)은 X_1의 분산을 의미
>> 
>> - 공분산 행렬의 dot(X_1, X_2)은 X_1과 X_2의 공분산을 의미 
>
> #### PCA(Principal Component Analysis)
>> PCA 알고리즘은 데이터 구조를 잘 살리면서 차원을 감소할 수있게 하는 방법
>> - 정사영 이후 데이터의 분포(=분산)이 제일 큰 것이 좋음
>> 
>> - 공분산 행렬로 선형 변환할 때, 주축에 대해 정사영하는 것이 좋음
>> 
>> - 선형변환의 주축을 eigenvector라 부름
>> 
>> - eigenvector를 찾는 다는 것은 “선형변환 이후 크기만 바뀌고 방향은 바뀌지 않는 벡터를 찾는것"
>> 
>> - 이는 정사영 후 데이터 분포(분산)가 가장 큰 결과를 얻기위해 eigenvector에 정사영
>> 
>> 3차원 데이터는?
>>  - 공분산 행렬을 고유값 분해하면 3개의 eigenvector가 나옴
>>  
>>  - 이 중 고유값이 큰순으로 2개의 eigenvector에 정사영 하면 2차원으로 축소됨
>>  
>> N차원 데이터는?
>> - 공분산 행렬을 고유값 분해하면 N개의 eigenvector가 나옴
>> 
>> - 이 중 고유값이 큰순으로 k개의 eigenvector에 정사영 하면 k차원으로 축소됨
>> 
> PCA
>> 공분산 행렬의 eigenvector, eigenvalue를 구하는 과정
>> 
>> 공분산 행렬의 고유 벡터는 데이터가 어떤 방향으로 분산되어 있는지 나타냄
>> 
>> PCA = Projecting data onto eigenvector of 공분산 행렬
