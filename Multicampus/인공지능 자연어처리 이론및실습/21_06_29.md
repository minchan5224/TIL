### 멀티캠퍼스 인공지능 자연어처리[NLP]기반 기업 데이터 분석.
- 8주차 1일 06/29
---
## 단어의 표현
- 자연어처리 텍스트마이닝
> #### 단어의 표현이 필요한 이유
> 자연어 자체(문자열)를 모델에 바로 사용할 수 없다.
> - 숫자화 시켜야함
> 
> ### 원핫-인코딩(One-Hot-Encoding)
> - 원핫-인코딩은 단어(word)를 숫자로 표현하고자 할 때 적용할 수 있는 간단한 방법론
>> 원숭이 = [1, 0, 0] (차원의 수, 예: 3차원)
>
> - 원핫-인코딩(One-Hot-Encoding)의 한계점
>> 1. 차원의 크기 문제(벡터로 표현한 단어 차원이 너무 큼 -> 연산이 낭비되어 모델 학습에 불리하게 적용)
>>> 원숭이, 바나나, 사과를 표현할 때
>>> - 단어의 수 만큼 차원이 필요함.(단어수가 많아진다면?)
>>> 
>>> - 2017년 표준국어대사전에 등재된 단어수 약 50만개 => 50만개의 차원이 필요하다.(원숭이=[1,0,0,0,0,0,0,....,0] => 50만 차원 벡터)
>>> 
>> 2. 의미를 담지 못하는 문제(단어 의미를 담지 못함 -> 분석을 효과적으로 수행할 수 없음)
>>> 원숭이, 바나나, 사과, 개 , 고양이를 표현할 때 
>>> - "원숭이, 사과" 코사인 유사도 : 0
>>> 
>>> - "원숭이, 바나나" 코사인 유사도 : 0
>>> 
>>> - "개, 고양이" 코사인 유사도 : 0
>>> 
>>> 원핫 벡터간 코사인 유사도는 모두 0, 따라서 의미를 분간하기 어렵다.
>>
>
> ### BoW(Bag of Words)
> 
> ### TF-IDF(Term Frequency-Inverse Document Frequency)
> 
> ### 단어 임베딩(Word Embedding)
> - 단어 임베딩은 단어의 의미를 간직하는 밀집 벡터(Dense Vector)로 표현하는 방법으로 임베딩이란 단어나 문장 각각을 벡터로 변환해 벡터 공간에 끼워 넣는다(embed)는 의미
> 
> 원숭이, 바나나, 사과를 표현할 때
>> - 벡터가 공간에 꽉차 있음
>>
>> - 새로운 단어 추가시 차원을 추가할 필요가 없음 
>>> => 차원을 줄일 수 있음
>>> 
>>> => 추후 분류나 예측 모델을 학습할 때 연산을 줄일 수 있는 이점을 갖음
>
> #### 단어 임베딩(Word Embedding)의 한계
>> - 벡터로 표현한 단어 차원이 너무 큼 => 밀집 벡터(Dense vector)로 해결
>> 
>> 사과 벡터는 어디에 표현되는 것이 맞는가 => 단어를 벡터로 표현하는 명확한 방법이 존재하지 않는다.
>> 
>> - 단어의 의미를 담지 못함 => ?
>> 
> #### 밀집 벡터를 만드는 방법
>> '같은 문맥에서 등장하는 단어는 유사한 의미를 지닌다'(단어의 의미는 곧 그 언어에서의 활용이다. - 비트켄슈타인)
>>
>> 1. 임의의 위치에 벡터 생성
>> 
>> 2. 같은 문맥이 등장하는 단어를 더 가까이 표현
>
> #### Word Representation
>> - Word Representation
>>> - Local Representation(Discrete representation) : 해당 단어 그 자체만 보고 값을 매핑하여 표현
>>>> - One-Hot vector
>>>> 
>>>> - N-gram
>>>> 
>>>> - Count Based
>>>>> - BoW(Bag of Words)
>>>>> 
>>>>> - TDM
>>> - Distributed Representation(Continuous representation) : 단어를 표현하기 위해 주변을 
>>>> - Prediction Based
>>>>> - Word2Vec
>>>>> 
>>>>> - FastText
>>>>> 
>>>>> - GloVe
>>>> - Count Based
>>>>> - Windows
>>>>>> - GloVe
>>>>> - Full document
>>>>>> - LSA
>>>>>
>>>>
>>>
>>
> ### 유사도 계산(Text Similarity)
> - ###### 텐서플로 2와 머신러닝으로 시작하는 저연어 처리 p118
> 
> #### 유클리디언 거리(Euclidean distance)
> - 벡터간 직진거리 계산
> 
> #### 코사인 유사도(Cosine Similarity)
> - 두 벡터간 유사도를 측정하는 방법 중 하나.
> 
> - 두 벡터 사이의 코사인을 측정(두 벡터간 각(코사인 유사도)을 이용한 유사도 측정)
>
> - 0도 = 1, 90도 = 0,  180도 = -1
>> => 1에 가까울수록 유사도가 높다.
>> 
>> = > 유사도가 높다는 것은 유사한 의미를 가짐을 의미.
>
> ### 유클리디안 vs 코사인
> - 상대적인 거리를 측정할 때
> 
> 만약 두 벡터를 비교할 때 벡터에 있는 값들의 전체 크기에 상관없이 상대적인 값들에만 관심이 있다면 코사인 유사도를 사용하며, 값의 절대적인 크기도 중요하다면 유클리디언 거리(좌표거리)를 구해야 한다.
> 
> #### 자카드 유사도(Jaccard index)
> - 문서 혹은 문장간 유사도 측정(겹치는 토큰의 비율)
> 
> #### 레벤슈타인 유사도(Levenshtein distance)
> - 두 문자열이 얼마나 다른지를 나타내는 거리 중 하나
> 
> ### n-Gram
> #### n-Gram 이란?
>> 복수개(n개) 단어를 보느냐에 따라 unigram, bigram, trigram등 으로 구분
>> 
>> 제한적으로 문맥을 표현할 수 있다.
>> 
>> 단어의 확률을 구하고자, 기준 단어의 앞 단어를 전부 포함해서 세지말고, 앞 단어 중 임의의 개수만 포함해서 세보는 것
>> - 그러면, 갖고있는 코퍼스에서도 해당 단어의 나열을 카운트할 확률이 높아진다.
>> 
>> 이 때 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram이다.(n-gram은 n개의 연속적인 단어 나열을 의미합니다.)
>> - 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 자연어 처리를 진행해보겠다는 것 
>> 
>> 예를 들어서 문장 An adorable little boy is spreading smiles이 있을 때, 각 n에 대해서 n-gram을 전부 구해보면 다음과 같다.
>>
>>> - [an adorable little boy is spreading smile](https://ai-information.blogspot.com/2019/02/n-gram.html)
>>> 
>>> unigrams : an, adorable, little, boy, is, spreading, smiles
>>> 
>>> bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles
>>> 
>>> trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles
>>> 
>>> 4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles
>> 
> ### n-Gram 활용
> - 연어 처리
>> ex) 금융 통화 위원회, 국회 의원, 고객 서비스
>> 
> - Language Modeling에 사용
>> - 분야(Domain)에 따라 단어들의 확률 분포는 다름
>>> (금융 분야는 금융 관련 용어가 많이 등장하고, 마케딩은 관련 용어가 많이 등장할 것임)
>> - 분야에 적합한 코퍼스를 사용하면 언어 모델의 성능이 높아질 수 있음
>>> (훈련에 사용되는 코퍼스에 따라 언어 모델의 성능이 달라짐 이는 언어 모델의 약점으로 분류되기도 함)
>>
> - 한국어 임베딩
---
## 문서의 표현(Document Representation)
- 문서의 표현(Document Representation / Sentence Representation)이란?
> 문서를 자연어처리를 위해 연산할 수 있도록 숫자로 표현하는 방법
> - 즉 문서를 벡터로 표현하는 방법.
>> 그러면 임베딩에 자연어 의미를 어떻게 함축할 수 있을까?
>> - 자연어의 통계적 패턴 정보를 통째로 임베딩해 넣는다.
>
> ### BoW(Bag of Words)
> - 문서 내 단어 출현 순서는 무시, 빈도수만 기반으로 문서를 표현하는 방법
> 
> #### BoW 생성 방법
>> 문서 1: 오늘 동물원에서 코끼리를 봤어
>> 
>> 문서 2: 오늘 동물원에서 원숭이에게 사과를 줬어
>> 
>> 1. 각 토큰에 고유 인덱스를 부여한다.
>> 
>> 2. 각 인덱스 위치에 토큰 등장 횟수를 기록한다.
>
> #### 활용과 한계
> - 활용
>> 정보 검색 분야
>>> 사용자의 질의에 가장 적절한 문서를 보여줄 때 질의를 BoW로 변화하고 질의와 검색 대상 문서 임베딩 간 높은 코사인 유사도를 가진 문서 출력
>>
> - 한계
>> 단어의 순서를 고려하지 않음
>> 
>> BoW는 Spare함. 벡터 공간의 낭비, 연산 비효율성 초래
>> 
>> 단어 빈도수가 중요도를 바로 의미하지 않음. 단어가 자주 등장한다고 중요한 단어는아님
>> 
>> 전처리가 매우 종요함. 같은 의미의 다른 단어 표현이 있을 경우 다른것으로 인식될 수 있음.
>> - 뉴스와 같이 정제된 어휘를 사용하는 매체는 좋으나, 소셜에서는 활용하기 어려움
>
> ### TDM(Term-Document Matrix)
> - BoW(Bag of Words) 중 하나
> 
> - 문서에 등장하는 각 단어 빈도를 행렬로 표현한 것
> 
> - TDM(Term-Document Matrix) : 단어-문서 행렬
> 
> - DTM(Document-Term Matrix) : 문서-단어 행렬
> 
> #### TDM(Term-Document Matrix) 의 한계
>> 단어의 순서를 고려하지 않음
>> 
>> TDM는 Spare함. 벡터 공간의 낭비, 연산의 비효율성 초래
>> 
>> 단어 빈도수가 중요도를 바로 의미하지 않음. the와 같은 단어는 빈번하게 등장하고 TDM에서 중요한 단어로 판단될 수 있음
>> - 이를 보완하기 위해 TF-IDF를 사용한다.
>
> ### TF-IDF(Term Frequency-Inverse Document Frequency)
> 단어 빈도-역문서 빈도
> 
> TDM 내 각 단어의 중요성을 가중치로 표현
> 
> TDM을 사용하는 것 보다 더 정확하게 문서 비교가 가능
>
> - tfidf(t, d, D) = tf(t, d) · idf(t, D)
>> tf(d, t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수
>> 
>> df(t) : 특정 단어 t가 등장한 문서의 수
>> 
>> idf(t) : df(t)의 역수
>> 
>> ```
>>  TF  | IDF  |  TF-IDF  |                    설명
>>  높     높       높        특정 문서에 많이 등장하고 타 문서에 많이 등장하지 않는 단어 (중요 키워드)
>>  높     낮        -        특정 문서에도 많이 등장하고 타 문서에도 많이 등장하는 단어
>>  낮     높        -        특정 문서에는 많이 등장하지 않고 타 문서에 많이 등장하지 않는 단어
>>  낮     낮       낮        특정 문서에 많이 등장하지 않고 타 문서에만 많이 등장하는 단어
>> ```
>> 
> #### TF-IDF 활용
> 단어와 문서사이의 연관성
> 
> 정보 검색(Information retrieval)
> - 검색어와 가장 관련이 있는 문서를 찾아 결과 제공 (ex. 검색 엔진에서 '인공지능')
> 
> 키워드 추출(Keyword Extraction)
> - TF-IDF 점수가 가장 높은 점수를 가지고 있는 단어가 그 문서를 대표하는 키워드
> 
> #### TF-IDF 적용 사례
> ```
> '4차 산업혁명, 다가오는 변화의 물결'
> 
> '[AI리포트] AI로봇기자가 이제는 시도 쓴다고?'     <-  인공지능
> 
> '포브스가 선정한 10년후 미래 유망 직업 Top 20'
> 
>                  ↓ TF-IDF
>
> '4차 산업혁명, 다가오는 변화의 물결' -> 4차 산업혁명
> 
> '[AI리포트] AI로봇기자가 이제는 시도 쓴다고?' -> 로봇기자
> 
> '포브스가 선정한 10년후 미래 유망 직업 Top 20' -> 직업
> ```
> #### [TF-IDF 가중치 계산, IDF에 로그를 사용하는 이유](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
> 
> #### TF-IDF 계산절차
> 1. 토큰 index 생성 
> - 각 토큰(단어)을 그대로 사용할 수 없기 때문에 토큰에 index를 부여
> 
> 2. TF계산
> - 각 토큰(단어)의 등장 빈도를 계산
> 
> 3. IDF 계산
> - 각 토큰(단어)의 문서 등장 빈도를 계산하여 역수를 취함
> 
> 4. TF-IDF계산
> - 계산한 TF와 IDF를 곱하여 TF-IDF를 계산
> 
> ### [잠재 의미 분석(Latent Semantic Analysis, LSA)](https://wikidocs.net/24949)
> 
> ### 그 외 문서의 표현
> 
> #### 단어-동시빈도 행렬(Term-Co-occurrence Matrix)
> - [단어간의 동시등장(co-occurrence)행렬](https://en.wikipedia.org/wiki/Co-occurrence)
> 
> #### 단어-문맥 행렬(Term-Context Matrix)
> - 단어-문맥 간의 동시등장(co-occurrence) 행렬
> 
> - 문맥은 사용자가 설정한 window의 크기로 결정
> 
> - 문맥 내 등장하는 단어의 빈도를 표기
> 
> - 한국어 임베딩
