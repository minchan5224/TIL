### 멀티캠퍼스 인공지능 자연어처리[NLP]기반 기업 데이터 분석.
- 8주차 1일 06/29
---
## 단어의 표현
- 자연어처리 텍스트마이닝
> #### 단어의 표현이 필요한 이유
> 자연어 자체(문자열)를 모델에 바로 사용할 수 없다.
> - 숫자화 시켜야함
> 
> ### 원핫-인코딩(One-Hot-Encoding)
> - 원핫-인코딩은 단어(word)를 숫자로 표현하고자 할 때 적용할 수 있는 간단한 방법론
>> 원숭이 = [1, 0, 0] (차원의 수, 예: 3차원)
>
> - 원핫-인코딩(One-Hot-Encoding)의 한계점
>> 1. 차원의 크기 문제(벡터로 표현한 단어 차원이 너무 큼 -> 연산이 낭비되어 모델 학습에 불리하게 적용)
>>> 원숭이, 바나나, 사과를 표현할 때
>>> - 단어의 수 만큼 차원이 필요함.(단어수가 많아진다면?)
>>> 
>>> - 2017년 표준국어대사전에 등재된 단어수 약 50만개 => 50만개의 차원이 필요하다.(원숭이=[1,0,0,0,0,0,0,....,0] => 50만 차원 벡터)
>>> 
>> 2. 의미를 담지 못하는 문제(단어 의미를 담지 못함 -> 분석을 효과적으로 수행할 수 없음)
>>> 원숭이, 바나나, 사과, 개 , 고양이를 표현할 때 
>>> - "원숭이, 사과" 코사인 유사도 : 0
>>> 
>>> - "원숭이, 바나나" 코사인 유사도 : 0
>>> 
>>> - "개, 고양이" 코사인 유사도 : 0
>>> 
>>> 원핫 벡터간 코사인 유사도는 모두 0, 따라서 의미를 분간하기 어렵다.
>>
>
> ### BoW(Bag of Words)
> 
> ### TF-IDF(Term Frequency-Inverse Document Frequency)
> 
> ### 단어 임베딩(Word Embedding)
> - 단어 임베딩은 단어의 의미를 간직하는 밀집 벡터(Dense Vector)로 표현하는 방법으로 임베딩이란 단어나 문장 각각을 벡터로 변환해 벡터 공간에 끼워 넣는다(embed)는 의미
> 
> 원숭이, 바나나, 사과를 표현할 때
>> - 벡터가 공간에 꽉차 있음
>>
>> - 새로운 단어 추가시 차원을 추가할 필요가 없음 
>>> => 차원을 줄일 수 있음
>>> 
>>> => 추후 분류나 예측 모델을 학습할 때 연산을 줄일 수 있는 이점을 갖음
>
> #### 단어 임베딩(Word Embedding)의 한계
>> - 벡터로 표현한 단어 차원이 너무 큼 => 밀집 벡터(Dense vector)로 해결
>> 
>> 사과 벡터는 어디에 표현되는 것이 맞는가 => 단어를 벡터로 표현하는 명확한 방법이 존재하지 않는다.
>> 
>> - 단어의 의미를 담지 못함 => ?
>> 
> #### 밀집 벡터를 만드는 방법
>> '같은 문맥에서 등장하는 단어는 유사한 의미를 지닌다'(단어의 의미는 곧 그 언어에서의 활용이다. - 비트켄슈타인)
>>
>> 1. 임의의 위치에 벡터 생성
>> 
>> 2. 같은 문맥이 등장하는 단어를 더 가까이 표현
>
> #### Word Representation
>> - Word Representation
>>> - Local Representation
>>>> - One-Hot vector
>>>> 
>>>> - N-gram
>>>> 
>>>> - Count Based
>>>>> - BoW(Bag of Words)
>>>>> 
>>>>> - TDM
>>> - Distributed Representation(Continuous representation)
>>>> - Prediction Based
>>>>> - Word2Vec
>>>>> 
>>>>> - FastText
>>>>> 
>>>>> - GloVe
>>>> - Count Based
>>>>> - Windows
>>>>>> - GloVe
>>>>> - Full document
>>>>>> - LSA
>>>>>
>>>>
>>>
>>
> ### 유사도 계산(Text Similarity)
> - ###### 텐서플로 2와 머신러닝으로 시작하는 저연어 처리 p118
> 
> #### 유클리디언 거리(Euclidean distance)
> - 벡터간 직진거리 계산
> 
> #### 코사인 유사도(Cosine Similarity)
> - 두 벡터간 유사도를 측정하는 방법 중 하나.
> 
> - 두 벡터 사이의 코사인을 측정(두 벡터간 각(코사인 유사도)을 이용한 유사도 측정)
>
> - 0도 = 1, 90도 = 0,  180도 = -1
>> => 1에 가까울수록 유사도가 높다.
>> 
>> = > 유사도가 높다는 것은 유사한 의미를 가짐을 의미.
>
> ### 유클리디안 vs 코사인
> - 상대적인 거리를 측정할 때
> 
> 만약 두 벡터를 비교할 때 벡터에 있는 값들의 전체 크기에 상관없이 상대적인 값들에만 관심이 있다면 코사인 유사도를 사용하며, 값의 절대적인 크기도 중요하다면 유클리디언 거리(좌표거리)를 구해야 한다.
> 
> #### 자카드 유사도(Jaccard index)
> - 문서 혹은 문장간 유사도 측정(겹치는 토큰의 비율)
> 
> #### 레벤슈타인 유사도(Levenshtein distance)
> - 두 문자열이 얼마나 다른지를 나타내는 거리 중 하나
> 
> ### n-Gram
> #### n-Gram 이란?
>> 복수개(n개) 단어를 보느냐에 따라 unigram, bigram, trigram등 으로 구분
>> 
>> 제한적으로 문맥을 표현할 수 있다.
>> 
>> 단어의 확률을 구하고자, 기준 단어의 앞 단어를 전부 포함해서 세지말고, 앞 단어 중 임의의 개수만 포함해서 세보는 것
>> - 그러면, 갖고있는 코퍼스에서도 해당 단어의 나열을 카운트할 확률이 높아진다.
>> 
>> 이 때 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram이다.(n-gram은 n개의 연속적인 단어 나열을 의미합니다.)
>> - 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 자연어 처리를 진행해보겠다는 것 
>> 
>> 예를 들어서 문장 An adorable little boy is spreading smiles이 있을 때, 각 n에 대해서 n-gram을 전부 구해보면 다음과 같다.
>>
>>> - [an adorable little boy is spreading smile](https://ai-information.blogspot.com/2019/02/n-gram.html)
>>> 
>>> unigrams : an, adorable, little, boy, is, spreading, smiles
>>> 
>>> bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles
>>> 
>>> trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles
>>> 
>>> 4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles
>> 
> ### n-Gram 활용
> - 연어 처리
>> ex) 금융 통화 위원회, 국회 의원, 고객 서비스
>> 
> - Language Modeling에 사용
>> - 분야(Domain)에 따라 단어들의 확률 분포는 다름
>>> (금융 분야는 금융 관련 용어가 많이 등장하고, 마케딩은 관련 용어가 많이 등장할 것임)
>> - 분야에 적합한 코퍼스를 사용하면 언어 모델의 성능이 높아질 수 있음
>>> (훈련에 사용되는 코퍼스에 따라 언어 모델의 성능이 달라짐 이는 언어 모델의 약점으로 분류되기도 함)
>>
> - 한국어 임베딩
