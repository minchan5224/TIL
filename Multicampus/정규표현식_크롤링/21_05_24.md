### 멀티캠퍼스 인공지능 자연어처리[NLP]기반 기업 데이터 분석.
- 3주차 1일 (5/24)

BS4 이용한 크롤링 실습문제 풀이.

---
> ## 실전 01번.
>> 네이버 영화랭킹
>>> ```Python
>>> import requests
>>> from bs4 import BeautifulSoup
>>> URL = '네이버 영화랭킹 url'
>>> response = requests.get(URL)
>>> soup = BeautifulSoup(response.text, "html.parser")
>>> result = soup.select(".list_ranking tr > td.title a")
>>> # class = list_ranking인 것의 tr태그의 자식중 td태그인 것중 class = title인 것의 <a>태그 가져온다.
>>> # td가 2개 임 ac는 순번 그림이고 title는 영화제목을 가지고 있다.
>>> for i, v in enumerate(result):
>>>     print(i+1, v.text) # 0부터 시작하기때문에 1을 더해준다.
>>> ```
>>
> ## 실습 01
>> 네이버 주가
>>> ```Python
>>> # 1번. 전체 들고오기.
>>> URL = "네이버 주가 url"
>>> response = requests.get(URL)
>>> soup = BeautifulSoup(response.text, "html.parser")
>>> result = soup.select("div.box_type_l tr")[2:]
>>> null_counter = 0
>>> #print(result)
>>> for i, j in enumerate(result):
>>>     if j.select(('tr > td[colspan="12"]')) == []:
>>>         print("{0} {1} {2}".format((i+1)-null_counter, j.select_one("a").text, j.select_one("td.number").text))
>>>     else :
>>>         null_counter += 1
>>> # j.select_one("a").text 이름 획득
>>> # j.select_one("td.number").text 가격 획득
>>> 
>>> # 2번. 전일대비 하락인 것만 가져오기
>>> null_counter = 0
>>> for i, j in enumerate(result):
>>>     if j.select_one(('td > img[alt="하락"]')) != None: # 하락인 경우
>>>     # 위 if문의 조건만 수정하면 상승인 경우와 그대로인 경우만 가져오는것도 가능
>>>         print("{0} {1} {2} {3}".format((i+1)-null_counter, j.select_one("a").text, j.select_one("td.number").text, >>> j.select_one('td > span').text.strip()))
>>>         # print(j.select_one('td > span').text) 전일비 획득
>>>     else :
>>>         null_counter += 1
>>> ```
>>
> ## 실습 02
>> 무신사 제품 정보
>>> ```Python
>>> import re
>>> URL = "무신사"
>>> response = requests.get(URL)
>>> soup = BeautifulSoup(response.text, "html.parser")
>>> result = soup.select("div.list-box.box li.li_box")
>>> for page_num in range(1,11): # 1~10번 페이지의 상품정보들을 전부 습득하기 위해.
>>>     print(page_num)
>>>     response = requests.get(URL+str(page_num))
>>>     soup = BeautifulSoup(response.text, "html.parser")
>>>     result = soup.select("div.list-box.box li.li_box")
>>>     for i in result: # 획득한 상품정보 출력.
>>>         print("브랜드 : {0}".format(i.select_one("p.item_title").text))
>>>         print("제품명 : {0}".format(re.sub("(.*/.* 배송)*(  )*(\n)*", "", i.select_one("p.list_info > a").text)))
>>>         print("가격 : {0}".format(i.select_one("p.price").text.split()[-1]))
>>> ```
>>
> ## 실습 03
>> 네이버 뉴스 크롤링(어제 생성된 기사만.)
>>> ```Python
>>> url_list = []
>>> t = True # while 문 조건
>>> new_sis = '신문사 상세주소.'
>>> page_url = '&page=' # 페이지 파라미터
>>> page_number = 0 # 페이지 번호.
>>> URL = "네이버 뉴스 url"
>>> 
>>> use_url = URL+new_sis+page_url+str(page_number) # 페이지 순회를 좀더 편하게 하기위해 url을 조각냄.
>>> 
>>> headers = {'User-Agent' : 'Mozilla/5.0'} # 헤더정보 기입하여 접근차단 방지
>>> response = requests.get(use_url, headers=headers)
>>> soup = BeautifulSoup(response.text, "html.parser")
>>> new_sis_y_day = soup.select_one("div.pagenavi_day > a")['href'] # 어제 날자 기사 페이지 url 획득
>>> #  위 코드만 조금 수정하면(반복문 처리) 어제 뿐 아니라 설정한 기간의 기사를 전부 크롤링 할 수 있다.
>>> 
>>> while t:
>>>     page_number += 1 
>>> # 1번 페이지부터 끝까지(마지막 페이지보다 큰 값이 들어가면 마지막페이지로 접속된다.)
>>> # 위 성질 이용해 페이지를 이동시킨다.
>>>     use_url = URL+new_sis_y_day+page_url+str(page_number)
>>>     response = requests.get(use_url, headers=headers)
>>>     soup = BeautifulSoup(response.text, "html.parser")
>>>     result = soup.select("div.list_body.newsflash_body > ul > li")
>>>     for i in result:
>>>         if i.select_one("dt > a")['href'] not in url_list: # 리스트 안에 해당 url이 없다면 아직 마지막 페이지가 아니다.
>>>             url_list.append(i.select_one("dt > a")['href'])
>>>         else : 
>>>             # 리스트 안에 해당 url이 존재한다면 마지막 페이지까지 수집을 한 뒤
>>>             # 마지막 페이지보다 더 큰 페이지 값을 기입해 다시 마지막 페이지로 돌아가 url을 수집한 것이기 때문이다.
>>>             t = False # 아래 for문이 끝나면 밖의 while 문 또한 종료시기키 위해
>>>             for j in url_list: # 획득한 기사들의 상세 페이지가 담긴 리스트를 하나씩 사용한다.
>>>                 try : # (1)try
>>>                     article_response = requests.get(j, headers=headers) # 상세페이지 정보 획득
>>>                     article_soup = BeautifulSoup(article_response.text, "html.parser") # 상세페이지 정보 획득
>>>                     article_title = article_soup.select_one("div.article_info > h3").text
>>>                     # 위는 기사 제목 획득
>>>                     article_text = article_soup.select_one("div#articleBodyContents").text.strip()
>>>                     # 위는 기사 내용 획득.
>>> 
>>>                     with open("test.txt",'a+', encoding='UTF-8')as f:
>>>                         f.write(article_title+"\n"+article_text+"\n\n")
>>>                         # 획득한 기사 제목과 내용을 txt파일에 저장한다.
>>>                 except AttributeError: # (1)except
>>>                     # 만약 기사 상세 페이지의 클래스가 다르다면 발생하는 오류 처리 위한 구문.
>>>                     try : # (2)try
>>>                         article_title = article_soup.select_one("div.news_headline > h4.title").text
>>>                         # 위는 기사 제목 획득
>>>                         article_text = article_soup.select_one("div#newsEndContents").text.strip()
>>>                         # 위는 기사 내용 획득.
>>> 
>>>                         with open("test.txt",'a+', encoding='UTF-8')as f:
>>>                             f.write(article_title+"\n"+re.sub("(\n)*", "", article_text)+"\n\n")
>>>                             # 획득한 기사 제목과 내용을 txt파일에 저장한다.
>>> 
>>>                     except AttributeError: # (2)except
>>>                         # 만약 또 기사 상세 페이지의 클래스가 다르다면 발생하는 오류 처리 위한 구문.
>>>                         # 총 3개의 기사 양식이 존재하는것을 알 수 있다.
>>>                         article_title = article_soup.select_one("h2.end_tit").text
>>>                         # 위는 기사 제목 획득
>>>                         article_text = article_soup.select_one("div.end_body_wrp > div#articeBody").text.strip()
>>>                         # 위는 기사 내용 획득.
>>> 
>>>                         with open("test.txt",'a+', encoding='UTF-8')as f:
>>>                             f.write(article_title.strip()+"\n"+re.sub("(\n)*", "", article_text)+"\n\n")
>>>                             # 획득한 기사 제목과 내용을 txt파일에 저장한다.
>>> 
>>>             break # for문 탈출시키기 (혹시모르는 오류 방지)
>>>
>>> with open("test.txt",'r', encoding='UTF-8')as f:
>>>     article_items = f.read()
>>> print(article_items) # txt파일 내용 출력.
>>> ```
>> 
