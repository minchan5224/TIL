### 멀티캠퍼스 인공지능 자연어처리[NLP]기반 기업 데이터 분석.
- 5주차 1일 06/07
---
> 머신러닝이 모든 문제를 해결 할 수 있는 것은 아니다.
> - 어떤 문제가 해결이 가능한지, 어떤 학습 방법을 써야 하는지 잘 판단하고 선택해야한다. 
>
> ## 데이터 전처리(개별변수 정제 방법)
> - 결측치(missing value) 처리
>> 결손값, NaN, Null값은 허용하지 않는다 : 고정된 다른 값으로 변환해야한다.
>> 
>> Feature값중 Null값이 얼마 되지 않는다면 평균값으로 대체
>> 
>> Null값이 대부분이면 Drpo
>>
>> Null값이 일정수준일때(해당 feature의 중요도가 높고, feature평균값으로 대체 불가능할 경우)
>> -예측 왜곡이 일어날 수 있다.(업무로직을 다시 고려)
>> 
> - 데이터 인코딩
>> 문자열 값을 입력값으로 허용하지 않는다. 
>> - 문자열 값을 인코딩해 숫자형으로 반환
>> 
>> 필드가 문자열인 값 (카테고리형 피처, 텍스트 피처)
>> -카테고리형 피처는 코드값으로 표현하는 것이 적합(코드형 숫자로 변환)
>> 
>> -텍스트 피처는 피처 벡터화 기법으로 벡터화 혹은 불필요한 피처일 경우 삭제(주민번호, 단순 ID)
>> 
>> 레이블 인코딩, 원-핫 인코딩
>> - 레이블 인코딩(카테고리 피처 -> 단순 숫자형으로 변환, scikit-learn class : LabelEncoder)
>> ```Python
>> from sklearn.preprocessing import LabelEncoder
>> 
>> items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']
>> 
>> # LableEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행
>> encoder = LabelEncoder()
>> encoder.fit(items)
>> labels = encoder.transform(items)
>> print('인코딩 변환값 :', labels)
>> ```
>> - 문제점
>>> 레이블 인코딩이 숫자의 크고 작음이 있는 값으로 면환되면서 특정 ML알고리즘에서 가중치가 더 부여되는 문제가 발생
>>> 
>>> 트리 계열의 알고리즘은 문제가 없으나 선형회귀 알고리즘은 문제 발생
>>> 
>>> 원-핫 인코딩이 이 문제를 해결
>> 
>> - 원-핫 인코딩(One Hot encoding, scikit-learn class : OneHotEncoder)
>> ``` Python
>> from sklearn.preprocessing import OneHotEncoder
>> import numpy as np
>> items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']
>> 
>> # 2차원 데이터로 변환, 바로 위의 레이블 인코딩에서 만든 값.
>> labels = labels.reshape(-1, 1)
>> 
>> # 원-핫 인코딩 적용
>> oh_encoder = OneHotEncoder()
>> oh_encoder.fit(labels)
>> oh_labels = oh_encoder.transform(labels)
>> 
>> print('원-핫 인코딩 데이터')
>> print(oh_labels.toarray())
>> 
>> print('원-핫 인코딩 데이터 차원')
>> print(oh_labels.shape)
>> 
>> #원- 핫 인코딩 : 판다스 get_dummies
>> import pandas as pd
>> df = pd.DataFrame({'item' : ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']})
>> pd.get_dummies(df)
>> 
>> ```
>>
> - 피처스케일링(feature scaling)
>> 두 변수 중 하나의 값이 너무 클 경우 학습을 시킬 때 가중치가 불균형 하게 반영이 될 수 있음, 학습 시 속도를 고려해서 값을 축소한다.
>> - 표준화(scikit-learn class : StandardScalar)
>>> 데이터의 피처 각각이 평균이 0이고 분산이 1인 가우시안 정규 분호를 가진 값으로 변환하는것
>>> ```Python
>>> # 표준화
>>> from sklearn.preprocessing import StandardScaler
>>> # StandardScaler객체 생성
>>> scaler = StandardScaler()
>>> # StandardScaler로 데이터 셋 변환, fit()과 transform()호출
>>> scaler.fit(iris_df)
>>> iris_scaled = scaler.transform(iris_df)
>>> 
>>> # transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame로 변환
>>> iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
>>> print('feature 들의 평균값')
>>> print(iris_df_scaled.mean())
>>> print('\nfeature 들의 분산 값')
>>> print(iris_df_scaled.var())
>>> ```
>> - 정규화(scikit-learn class : MinMaxScalar)
>>> 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해 주는 개념
>>> ```Python
>>> # 정규화
>>> from sklearn.preprocessing import MinMaxScaler
>>> # MinMaxScaler객체 생성
>>> scaler = MinMaxScaler()
>>> # MinMaxScaler로 데이터 셋 변환. fit()과 transform()호출.
>>> scaler.fit(iris_df)
>>> iris_scaled = scaler.transform(iris_df)
>>> 
>>> # transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame로 변환
>>> iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
>>> print('feature 들의 최소 값')
>>> print(iris_df_scaled.min())
>>> print('\nfeature 들의 최대 값')
>>> print(iris_df_scaled.max())
>>> 
>>> # 학습데이터와 테스트 데이터의 스케일링 변환시 유의점
>>> from sklearn.preprocessing import MinMaxScaler
>>> import numpy as np
>>> # 학습 데이터는 0부너 10까지 테스트 데이터는 0부터 5까지 값을 다지는 데이터 세트로 생성
>>> # Scaler클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경 
>>> train_array = np.arange(0, 11).reshape(-1, 1)
>>> test_array = np.arange(0, 6).reshape(-1, 1)
>>> 
>>> # 최솟값 0 최대값 1로 변환하는 MinMaxScaler객체 생성
>>> scaler = MinMaxScaler()
>>> #fit() 하게 되면 train_array 데이터 최솟값이 0, 최댓값이 10으로 설정
>>> scaler.fit(train_array)
>>> # 1/10 Scale로 train_array 데이터 변환함, 원본 10->1 로 변환됨.
>>> train_scaled = scaler.transform(train_array)
>>> 
>>> print('원본 train_array 데이터',np.round(train_array.reshape(-1),2))
>>> print('Scale된 train_array 데이터',np.round(train_scaled.reshape(-1),2))
>>> 
>>> # scaler.fit(test_array) 이미 train할때 fit() 했기때문에 test에선 할 필요 없음, 처음 fit 했을때 이미 기준점 잡았기때문
>>> # transform 만 해주면 된다.
>>> # 1/5 Scale로 test_array 데이터 변환함, 원본 10->1 로 변환됨.
>>> test_scaled = scaler.transform(test_array)
>>> 
>>> print('원본 test_array 데이터',np.round(test_array.reshape(-1),2))
>>> print('Scale된 test_array 데이터',np.round(test_scaled.reshape(-1),2))
>>>  ```
>> 학습 데이터와 테스트 데이터의 스케일린 변환시 유의점
>>> - 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터 분리
>>>
>>> - 위 항목이 여의치 않다면 테스트 데이터 변환 시 fit()이나 fit.transform() 적용하지 않고 학습 데이터로 이미 fit() 된 Scaler 객체를 이용해 transform()으로 변환
> 
> ## 타이타닉 생존자 예측
> - 이진 분류 모델
>> ```
>> 1. 문제정의
>> 2. 데이터 수집
>> 3. 데이터 전처리
>> 4. ML 알고리즘 적용
>> 5. 성능평가
>> ```
