### 멀티캠퍼스 인공지능 자연어처리[NLP]기반 기업 데이터 분석.
- 7주차 2일 06/23
#### 데이터 분석 이론및실습 통합 정리
---
### 머신러닝 종류
> 컴퓨터가 경험을 통해 학습하는 과정을 거치면서 입력되지 않은 정보에 대한 문제 해결/의사 결정 등을 할 수 있는 인공지능의 한 분야.
> - 서포트백터머신, 의사결정트리, 앙상블, 인공신경망 등.
> 
> 사람의 감독 하에 훈련하는 것인지 아닌지 : 지도, 비지도, 준지도, 강화학습
> 
> 실시간으로 점진적인 학습을 하는지 아닌지 : 온라인 학습과 배치학습
> 
> 기존과 새 데이터 포인트 비교인지 아니면 훈련 데이터로 예측 모델을 만드는지 : 사례 기반 학습과 모델 기반 학습
### 강화학습 비지도학습
> - #### 지도학습(Supervised Learning)
>> 분류, 회귀, 예측이 포함.
>> 
>> 정답을 넣어 줘야하며 정확한 데이터(정확한 레이블)가 필요하다.
>>
>> 학습 : 데이터, 정답 
>> - 학습시 데이터와 정답을 제공해 주어야 한다.
>
> - #### 비지도학습(Unsupervised Learning)
>> 정답을 넣어주지 않아도 되며 데이터의 특성과 특징이 중요하다.
>> 
>> 학습은 패턴, 차이점을 구별하여 이루어진다.
>
> - #### 강화학습/목표 지향학습(Reinforcement Learning)
>> 보상을 제공해야하며 행동에 따른 인과관계가 중요하다.
>> 
>> 경험 데이터 기반으로 학습이 이루어진다.
### 머신러닝으로 할 수 있는 것.
> ```
>      분류   |   회귀    |    예측
>     이상 값 감지  |  그룹화
>               강화학습
> ```
> - #### 분류(Classification)
>> 동물, 음식등 다양한 사물들의 학습
>> 
>> 스팸 필터 학습
>> 
>> 손 글씨 학습
>> 
>> 꽃/나무의 종류들을 학습
>>
>> 음악에 대한 분류
>> 
>> 미술품에 대한 분류
>>
>> 사람의 감정에 대한 분류
>
> - #### 회귀(Regression)
>> 설탕섭취량에 따른 뭄무게의 변화
>> 
>> 수면시간에 따른 일의 만족도
>> 
>> 학력에 따른 월간 독서량
>> 
>> 공부시간에 따른 시험 성적
>> 
>> 맥주 구매에 따른 기저귀 구매 비율
>> 
>> 온도에 따른 레모네이드 판매량
>> 
>> 자동차 속도에 따른 충돌 시 사망 확률
>
> - #### 예측(Forecast)
>> 올해 아보카도의 수화량 예측
>> 
>> 요일별 비행기 판매율
>> 
>> 자전거 수요 예측
>> 
>> 주말에 비가 올 확률 예측
>> 
>> 내년에 태어날 신생인구 예측
>> 
>> 다음달 주가 예측
>>
>> 서울의 10년후 주택 가격 예측
> 
> - #### 이상값 감지(Anomaly Detection)
>> 신용카드 거래 감지
>> 
>> 비 정상 세포 감지
>> 
>> 센서 이상 값 감지
>
> - #### 그룹화(Clustering)
>> SNS 친구 분석
>> 
>> 비슷한 그림 묶음
>> 
>> 카드 고객들의 레밸링(사용금액)
>> 
>> 취미가 비슷한 사람들의 묶음
>
> - #### 강화학습(Reinforcement Learning)
>> 게임(알파고, 인공지능 체스)
>> 
>> 로봇(로봇의 움직임, 공장 자동화)
>> 
>> 자율주행(드론, 자동차, 비행기)
>> 
> ```
> x : 고객들의 개인 정보 및 금융 관련 정보, y : 대출 연체 여부 -> 대출 연체자 예측 탐지 모델, 대출 연체 관련 주요 feature추출
> x : 게임 유저들의 게임 내 활동 정보, y : 게임 이탈 여부/ 어뷰징 여부 -> 이상 탐지 모델
> x : 숫자 손 글씨 데이터, y : 숫자 라벨(0~9) -> 숫자 이미지 분류 모델
> x : 상품 구매 고객 특성 정보 -> 군집화를 통한 고객 특성에 따른 segmentation
> x : 고객들의 상품 구매 내역 -> 매장내 상품 진열 위치 리뉴얼을 통한 매출 증대
> x : 쇼핑몰 페이지 검색 및 클릭 로그 기록 -> 맞춤 상품 추천 시스템
> x : SNS 데이터 및 뉴스 데이터 -> 소셜 및 사회 이슈 파악
> ```
### 회귀/분류 종류
> - #### 회귀
>> 선형회귀분석(Linear Regrression)
>>> 독립변수와 종속변수가 선형적인 관계가 있다라는 가정하에 분석
>>> 
>>> 직선을 통해 종속변수를 예측하기 떄문에 독립변수의 중요도와 영향력을 파악하기 쉽다.
>>> 
>>> 연속형 data의 input으로 연속형 output를 가진다.
>>> 
>>> Ridge
>>> - L2 Regression
>>> Lasso
>>> - L1 Regression
>>> ElasticNet
>>
>
> - #### 분류
>> - 로지스틱 회귀(RogisticRegression)
>>> 연속형 data의 input로 이산형 output를 가진다.
>>> 
>>> 오차값 : cross entropy
>>
>> - 의사 결정나무(Decision Tree)
>>> 독립 변수의 조건에 따라 종속변수를 분리
>>> 
>>> 이해하기 쉬우나 overfitting이 잘 일어난다.
>>
>> - 랜덤포레스트(RandomForest)
>
### 과대적합의 염려가 크지 않으며 데이터가 적을때 쓸수있는기법
> - Kfold, Stratified K 폴드, cross_val_score() 등의 교차검증.
> 
> - ModelSelection모듈
> 
### 사전분석 명령어
> 아마 .info(), .describe()이라 생각된다. DataFrame의 변수 속성, 기본적 수학 연산내용을 볼 수 있다.
> 
> ```Python
> import matplotlib.pyplot as plt
> import seaborn as sns
> ```
> 등 그래프를 그려서 확인하는 방법 등. 
> 
> 모델 훈련 전 각 변수마다 어떤 상관관계가 있는지 확인하고 버릴건 버리고 하는것이 좋을것 같다.
>
### 데이터분석방법(셀로우 러닝 (디시전트리, 로지스틱 회귀, 리니어 들을 뭉쳐서 하는거
> 앙상블(배깅, 부스팅)
### 전처리 할때 쓰이는 코드
> #### 결측치 처리
>> Null값이 얼마 되지 않는다면 feature의 평균값 등으로 대체
>> 
>> Null 값이 대부분이라면 Drop
>> 
>> Null 값이 일정수준 이라면 중요도에 따라 업무로직을 다시 고려한다.
>> 
> #### 데이터 인코딩
> 문자열 값을 입력으로 처리하지 않기때문에 문자열 값을 인코딩해 숫자형으로 변환한다.
>> 레이블 인코딩 : 카테고리 피처 -> 코드형 숫자로 변환.(LabelEncoder)
>> ```Python
>> from sklearn.preprocessing import LabelEncoder
>> # LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행.
>> encoder = LabelEncoder()
>> encoder.fit(items)
>> labels = encoder.transform(items)
>> ```
>> - 문제점.
>>> 레이블 인코딩이 숫자의 크고 작음이 있는 값으로 변환되며 특정 ML알고리즘에서 가중치가 더 부여되는 문제 발생
>>> 
>>> 트리 계열의 알고리즘은 문제가 없지만 선형회귀 알고리즘에서 문제가 발생하며 이 문제는 원-핫 인코딩을 통해 해결 가능하다.
>>> 
>>
>> 원-핫 인코딩 : 새로운 피처를 추가해 고유 값 컬럼만 1로 표기하고 나머지는 0으로 처리하는 방식(OneHotEncoder)
>> ```Python
>> from sklearn.preprocessing import OneHotEncoder
>> # LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행.
>> encoder = LabelEncoder()
>> encoder.fit(items)
>> labels = encoder.transform(items)
>> labels = labels.reshape(-1,1) # 2차원 데이터로 변환.
>> 
>> # OneHotEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행.
>> oh_encoder = OneHotEncoder()
>> oh_encoder.fit(labels)
>> oh_labels = oh_encoder.transform(items)
>> 
>>
>> # 판다스를 이용해서도 가능하다.
>> import pandas as pd
>> df = pd.DataFrame(item)
>> pd.get_dummies(df)
>> ```
>> 
> #### 피처 스케일링
> 두 변수중 하나의 값이 너무 클 경우 학습 시킬때 가중치가 불균형 하게 반영이 될수 있다.
> 
> 학습시 속도를 고려해 값을 축소시킨다.
> 
>> 표준화(Standardization)
>> - 데이터의 피처 각각이 평균이 0이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환하는것.
>> ``` Python
>> from sklearn.preprocessing import StandardScalar
>> 
>> sacler = StandardScalar() # 객체 생성
>> # 데이터셋 변환. fit()과 transform()호출
>> sacler.fir(iris_df)
>> iris_scaled = scaler.transform(iris_df)
>> 
>> iris_df_scaler = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
>> 
>> iris_df_scaler.mean() # feature들의 평균 값
>> 
>> iris_df_scaler.var() # feature들의 분산 값
>> ```
>> 
>> 정규화(Normalization)
>> - 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해 주는 개념
>> ``` Python
>> from sklearn.preprocessing import MinMaxScaler
>> 
>> sacler = MinMaxScaler() # 객체 생성
>> # 데이터셋 변환. fit()과 transform()호출
>> sacler.fir(iris_df)
>> iris_scaled = scaler.transform(iris_df)
>> 
>> iris_df_scaler = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)
>> 
>> iris_df_scaler.min() # feature들의 최소 값
>> 
>> iris_df_scaler.max() # feature들의 최대 값
>> ```
>> 학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점
>> - 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리
>> 
>> - 위 항목이 여의치 않다면 테스트 데이터 변환시에는 fit()이나, fit_transform()을 적용하지 않고 학습 데이터로 이미 fit된 scaler객체를 이용해 transform()으로 변환.
> 
### 개별 변수에대한 사전 정제 방법 결측치 등
> #### 바로 위에서 작성한 결측치 처리, 데이터 인코딩, 피처스케일링 참고.
### 원핫인코딩 명령어 순서
> #### 마찬가지로 바로 위에서 데이터 인코딩 파트의 원핫 인코딩 참고.
### [분류 성능 평가 지표](https://sumniya.tistory.com/26)
> 
### [회귀 성능 평가 지표](https://pro-jm.tistory.com/31)
> 
### 결정 트리(Decision Tree) 하이퍼 파라미터 종류
> #### max_depth
>> - 트리의 최대 길이를 규정
>> 
>> - 디폴트로는 None, None로 설정하면 완벽하게 클래스 결정 값이 될 때 까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 개수가 min_samples_split보다 작아질 때까지 계속 깊이를 증가시킨다.
>> 
>> - 깊이가 깊어지면 min_samples_split 설정대로 최대 분할아여 과적합 할 수 있으므로 적절한 값 제어가 필요하다.
>
> #### max_features
>> - 최적의 분할을 위해 고려할 최대 피처 개수, 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할을 수행한다.
>> 
>> - int형으로 지정하면 대상 피처의 개수, float형으로 지정하면 전체 피처중 대상 피처의 퍼센트다.
>> 
>> - 'sqrt'는 전체 피처중 sqrt(전체 피처 개수), 즉 루트(전체 피처) 개수만큼 선정
>> 
>> - 'auto'로 지정하면 sqrt와 동일
>> 
>> - 'log'는 전체 피처중 log2(전체 피처 개수) 선정
>> 
>> - 'None'은 전체 피처 선정
> 
> #### min_samples_split
>> - 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는데 사용된다.
>> 
>> - 디폴트는 2 이고 작게 설정할 수록 분할 되는 노드가 많아져 과적합 가능성이 증가한다.
>> 
>> - 과적합을 제어. 1로 설정할 경우 분할되는 노드가 많아져 과적합 가능성이 증가한다.
>
> #### min_samples_leaf
>> - 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수
>> 
>> - min_samples_split 과 유사하게 과적합 제어용도. 하지만 비대칭적(imbalanced)데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요
>
> #### max_leaf_nodes
>> - 말단 노드(Leaf)의 최대 개수
>
### [학습 최적화할때 쓰는 방법(모멘텀, Adam, 경사 하강법)](https://ganghee-lee.tistory.com/24)
> 
### [이미지 분석에 가장 적합한 것](https://3months.tistory.com/512)
> 분류(Classification) : KNN?, 로지스틱 회귀?
> 
### 딥러닝에서 가장 단순한 형태의 신경망([퍼셉트론](https://ko.wikipedia.org/wiki/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0))
> 
### 딥러닝 가장 마지막 노드에서 쓰이는 활성화 함수
> 다중 분류 문제의 경우에는 softmax 함수를 출력으로 가장 먼저 생각 할 것
>  
> 0~1 사이로 한 사건의 확률을 모델링 할 경우에는 Sigmoid 함수를 가장 먼저 생각해 볼 것
>
> - [시그모이드 함수(sigmoid)](https://reniew.github.io/12/)
>
> - [소프트맥스 함수(softmax)](https://gooopy.tistory.com/53?category=824281)
> 
### [이진분류(Binary Classification)](https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=guseod24&logNo=221495157749&proxyReferer=)
> ```Python
> from keras import models
> from keras import layers
> 
> model = models.Sequential()
> model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
> model.add(layers.Dense(16, activation='relu'))
> model.add(layers.Dense(1, activation='sigmoid'))
> ```
> 영화리뷰 분석(긍, 부)
### 순차적인 데이터 학습에 특화된 인공신경망(RNN)
> Recurrent : 몇번이나 반복해서 일어나는 일
> 
> - 순환하는 신경망
>> 반복해서 돌아감
>> 
>> 순환하기 위해선 닫힌 경로(순환하는 경로)가 필요
>> 
>> 과거 정보를 유지하면서(기억하면서) 새롭게 업데이트할 수 있도록 구성
### [LSTM](https://wikidocs.net/106473)
> ```Python
> model = Sequential()
> model.add(LSTM(units=10, activation='relu', return_sequences=True, input_shape=(window_size, data_size)))
> model.add(Dropout(0.1))
> model.add(LSTM(units=10, activation='relu'))
> model.add(Dropout(0.1))
> model.add(Dense(units=1))
> model.summary()
> 
> model.compile(optimizer='adam', loss='mean_squared_error')
> model.fit(train_x, train_y, epochs=60, batch_size=30)
> pred_y = model.predict(test_x)
> ```
> 위와같이 모델 선언하고 층 만들어주고 세부 설정하고 돌린다.
> 
> Dence에서 이름만 바뀐거라 생각하자.
> 
### 딥러닝 환경설정 loss, optimaizer ( model.compile()안에 셋팅하는 값들 
> ```python
> model.compile(optimizer='rmsprop',
>             loss='binary_crossentropy',
>             metrics=['accuracy'])
> ```
> 옵티마이저, 손실함수, 매트릭스.
