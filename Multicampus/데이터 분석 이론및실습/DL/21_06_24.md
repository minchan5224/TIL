### 멀티캠퍼스 인공지능 자연어처리[NLP]기반 기업 데이터 분석.
- 7주차 3일 06/24
---
## CNN들어가며.
>이미지 배열을 딥러닝에선 '텐서'라 한다.
>
> Image 색상 모델(Grayscale, RGB, HSV)
> - Grayscale : Height * Width 형태의 2차원 배열
> - Color : Height * Width * Charnel(R G B 가 들어있다.) 형태의 3차원 배열(텐서플로우는 채널이 마지막에 온다.)
> 
> RGM Image 배열과 픽셀값.
> - charnel로 관장한다.
> - R에 대한 픽셀값 + G에 대한 픽셀값 + B에 대한 픽셀값 = 이미지
> 
> 정형데이터, Gray 이미지, RGB배열 차원
> - 정형 데이터 : 2차원
> - Grayscale 이미지 : 2차원 (여러개의 이미지일 경우 3차원)
> - RGB 이미지 : 3차원 (여러개의 이미지일 경우 4차원)
> 
---
## 합성곱 신경망(CNN) 소개
> #### 일반 Dense Layer에서 Image Classification을 사용하지 않는다.
> - MNIST는 분류 대상이 이미지 중앙에 고정, 배경이미지가 0으로 명확하게 정리 되어있어서 사용했다.
> 
> #### 일반적인 이미지는 분류 대상이 어디에 있는지 모른다.
> - 고정적인 방식을 가지는 Dense Layer로 처리하지 않는다.
> 
> #### 이미지의 크기가 작지 않다(MNIST = 28 * 28, 일반적인 이미지는 훨씬크다) : Flatten 적용이 쉽지않다. 
> <br>
> 
> #### CNN은
>> Classification에 맞는 최적의 Feature를 추출
>> 
>> 최적의 Feature 추출을 위한 최적 Weight값 계산
>> 
>> 최적 Feature 추출을 위한 필터(필터 Weight)값을 계산
>
> #### 이미지 필터링 -> 핵심 Feature 추출하기 위해.
> 보통 정방 행렬을 원본 이미지에 순차적으로 슬라이딩 하며 새로운 픽셀값을 만들면서 적용한다.
>
> Convolution 연산
> 
>> 이미지의 좌상단 부터 1point씩 sliding하면서 Convolution연산을 순자적으로 수행
>>
>> 개별 Convolution 연산은 filter에 매핑되는 원본 이미지 배열과 filter배열을 Element-wise하게 곱셈을 적용한 뒤 합을 구함.
>
> Stride -> Convolution연산시 몇 point씩 슬라이딩 할 것이냐.
>> ##### 특징
>> stride는 입력 데이터에 Conv filter를 적용할 때 Sliding Window가 이동하는 간격을 의미
>>
>> 기본은 1이며, 2를(2 픽셀 단위 슬라이딩 윈도우 이동) 적용하여 입력 Feature map대비 출력 feature map의 크기를 대략 절반으로 줄인다.
>>
>> stride를 키우면 공간적인 feature특성을 손실할 가능성이 높아지지만 이것이 중요 feature들의 손실을 반드시 의미하지는 앖으며 오히려 불필요한 특성을 제거하는 효과를 가져올 수 있다. 또한 Convolution연산 속도를 향상 시킨다.
> 
> #### Padding 개요
> Filter를 적용하여 Conv연산 수행 시 출력 Feature Map이 입력 Feature Map 대비 계속적으로 작아지는 것을 막기 위해 적용
>
> Filter적용 전 보존하려는 Feature map 크기에 맞게 입력 Feature Map의 좌우 끝과 상하 끝에 각각 열과 행을 추가한 뒤 0값을 채워 입력 Feature map사이즈를 증가 시킨다.
>
> 아래 4X4 Feature Map에 2X2 Filter를 적용 시 출력 Feature Map은 3X3이 된다. 입력 Feature Map와 출력 Feature Map의 크기를 맞추기 위해 좌우 끝과 상하 끝에 0값을 채워 6X6 Feature Map으로 변경. 그리고 2X2 Filter를 적용하여 Padding 적용 전 원본 Feature Map크기와 동일한 4X4 Feature Map을 출력.
>
>
> #### Padding 특징
> CNN Network가 깊어질수록(Conv적용을 많이 할수록) Feature Map크기가 줄어드는 현상을 막을 수 있다.
>
> 모서리 주변(좌상단, 우상단, 좌하단, 우하단)의 Conv 연산 횟수가 증가되어 모서리 주변 Feature들의 특징을 보다 강화하는 장점이 있다.
>
> Zero Padding의 영향으로 모서리 주변에 0값이 입력되어 Noise가 약간 증가 되는 우려도 있지만 큰 영향은 없다.
>
> Keras에서 Conv2D()인자로 padding='same'을 넣어주면 Conv연산 시 자동으로 입력 Feture map의 크기를 출력. Feature Map에서 유지할 수 있게 padding면적을 계산하여 적용함. padding='valid'적용하면 별도의 padding을 적용하지 않고 Conv연산 수행
>
> #### Pooling 개요
> Conv 적용된 Feature Map의 일정 영역 별로 하나의 값을 추출하여 (주로 Max, Average적용) Feature Map의 사이즈를 줄임(sub sampling). 일반적으로 Polling크기의 Stride를 동일하게 부여하여 모든 값이 한번만 처리될 수 있도록 함.
>
> 일정 영역에서 가장 큰 값 또는 평균 값을 추출하므로 위치의 변화에 따른 Feautre 값의 변화를 일정 수준 증가시킬 수 있다.
>
> 보통은 Conv -> ReLU Activation 적용후 Activation Map에 Polling적용
>
> #### Pooling 특징
> Polling은 비슷한 Feature들이 서로 다른 이미지에서 위치가 달라지면서 다르게 해석되는 현상을 중화시켜준다.
>
> Feature Map의 크기를 줄이므로 Commputation 연산 성는 향상(Conv적용 보다 computation이 더 간단)
>
> Max Polling의 경우 보다 Shape한 feature값을 (Edge등) 추출하고 Average Pooling의 경우보다 Smooth한 feature값을 추출
>
> 일반적으로는 Shape한 feature가 보다 Classfication에 유리하여 Max Polling이 더많이 사용됨.
>
> #### strides/padding, pooling
> Stride를 증가시키는 것과 Pooling 모두 출력 Feature Map의 크기를 줄이는데 사용, Conv연산을 진행하면서 점차 Feature Map의 크기를 줄이면 위치의 변화에 따른 Feature 값의 영향도를 줄여(Spatital invariance) Generalization, 오버피팅 감소 등의 장점을 얻을 수도 있음.
>
> Pooling의 경우 특정 위치의 feature값이 손실 되는 이슈 등으로 인하여 최근 Advanced CNN에서는 많이 사용되고 있지 않음.
>
> 과거 LeNet, AlexNet, VGG의 경우는 CNN(Stride/Padding)->Activation->Pooling으로 이어지는 전형적인 구조를 갖추었으니 이후 발표되는 논문 등에서 Stride로 feature map크기를 줄이는 것이 Pooling보다 더 나은 성늘을 보인다는 연구 결과를 발표하기 시작
>
> ResNet부터 이어지는 최근 CNN에서는 최대한 Pooling을 자제하고 Strinde를 이용해 Network를 구성하는 경향이 강해 짐.
