### 멀티캠퍼스 인공지능 자연어처리[NLP]기반 기업 데이터 분석.
- 6주차 2일 06/16
---
> ## 활성화 함수
> - 시그모이드 함수(sigmoid)
>> Sigmoid = Logistic
>> 
>> 선형인 멀티퍼셉트론에서 비선형값을 얻기 위해 사용
>> 
>> - 한계
>>> 기울기 소실 문제 :  input 값이 일정 이상 올라가면 미분값이 거의 ()에 수렴
>>> 
>>> 함수값 중심이 0이 아님 에 따라 학습이 느려진다.
>>> 
>>> exp함수 사용시 비용이 큼
>> 
>> - hard_sigmoid
>>> x < -2.5 일때 0
>>> 
>>> x > 2.5 일때 1
>>> 
>>> 그 외에는 0.2 * x + 0.5
>> 
> - tanh함수(tanh)
>> - 하이퍼볼릭탄젠트
>>> 함수값 중심을 0으로 옮겨 최적화 과정이 느려지는 문제 해결
>>> 
>>> 여전히 기울기 소실 문제 발생
>> 
> - ReLU함수 계열
>> - Advanced Activations Layers 사용
>>> from keras.layers import "활성화 함수 이름"
>>> 
>>> 이전 노드에서 활성화 함수를 정의하지 않고 바로 이어서 활성화 함수를 정의
>>
>> - ReLU(relu 또는 ReLU)
>>> 학습이 빠르고 구현이 매우 간단하다.
>>> 
>>> x < 0인 값들에 대해 뉴런이 죽을수 있다는 단점 존재
>>> 
>>> ReLU(max_value=None, negative_slope=0.0, threshold=0.0)
>> - ThresholdedReLU
>>> 0 대신 임계치(theta) 사용
>>> 
>>> ThresholdedReLU(theta=1.0)
>> - LeakyReLU
>>> Dying ReLU 현상을 해결하기 위해 나온 함수 : f(x) = max(alpha * x, x)
>>> 
>>> 음수의 x값에 대해 미분값이 0이 되지 않는다는 점을 제외하면 같은 특성
>>> 
>>> LeakyReLU(alpha-0.3)
>> - PReLU
>>> Leaky ReLU와 유사하지만 새로ㅜㄴ 파라미터를 추가하여 x<0 에서의 기울기 학습
>>> 
>>> f(x) = max(a * x, x)
>>> 
>>> PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None
>> - Exponential Linear Unit(ELU)
>>> 비교적 최신에 나온 함수.
>>> 
>>> ReLU의 모든 장점 포함
>>> 
>>> Dying ReLU 문제 
>>> 
>>> 출력값이 거의 zero-centered
>>> 
>>> exp계산하는 비용 발생
>>> 
>>> ELU(alpha=1.0)

> ## [옵티마이저](https://velog.io/@yookyungkho/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%98%B5%ED%8B%B0%EB%A7%88%EC%9D%B4%EC%A0%80-%EC%A0%95%EB%B3%B5%EA%B8%B0%EB%B6%80%EC%A0%9C-CS231n-Lecture7-Review)
> - 손실함수의 기울기를 전파해 가며 가중치를 조정해 나가는 방법.
> 
